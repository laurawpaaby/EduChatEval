{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>This package offers a framework for researchers to log and classify interactions between students and LLM-based tutors in educational settings. It supports structured, objective evaluation through classification, simulation, and visualization utilities, and is designed for flexible use across tasks of any scale. The framework supports both researchers with pre-collected datasets and those operating in data-sparse contexts. It designed as a modular tool that can be integrated at any stage of the evaluation process.</p> <p>The package is designed to:</p> <ul> <li>Synthesize a labeled classification framework using user-defined categories </li> <li>Simulate multi-turn student\u2013tutor dialogues via role-based prompting and structured seed messages</li> <li>Wrap direct student-tutors interaction with locally hosted LLMs through a terminal-based interface </li> <li>Fine-tune and apply classification models to label conversational turns</li> <li>Visualize dialogue patterns with summary tables, frequency plots, temporal trends, and sequential dependencies</li> </ul>"},{"location":"#user-guides-and-api","title":"User Guides and API","text":"<p>To run the full pipeline, the package requires integration with LM Studio (for local model hosting) and the Hugging Face ecosystem. Step-by-step tutorials are provided in the User Guide section, covering setup, configuration, and usage across all modules\u2014from data generation to classification and visualization.</p> <p>Detailed information on the available classes, functions, and configuration options can be found in the API reference.</p> <p>Be aware, that the package currently requires <code>Python 3.12</code> due to version constraints in core dependencies, particularly <code>outlines</code>.</p>"},{"location":"about/","title":"About","text":""},{"location":"about/#project","title":"Project","text":"<p>This package was developed as part of a master\u2019s thesis in Cognitive Science at Aarhus University. It provides a modular framework for systematically capturing and analyzing interactions between students and LLM-based chatbots in educational settings.</p> <p>The core contribution is supporting quantitative, interaction-level evaluation by turning dialogue data into structured, labeled outputs. By combining tools for classification, simulation, and visualization, the package enables data-driven reproducible analysis of chatbot-assisted learning and is intended for use in both experimental and applied research contexts.</p> <p>The full source code for the <code>EduChatEval package</code> is available on GitHub and is licensed under MIT License</p>"},{"location":"about/#contact","title":"Contact","text":"<p>For questions or collaboration inquiries, feel free to reach out to Laura W Paaby.</p>"},{"location":"about/#acknowledgements","title":"Acknowledgements","text":"<p>This project builds on existing tools and ideas from the open-source community. While specific references are provided within the relevant scripts throughout the repository, the key sources of inspiration are also acknowledged here to highlight the contributions that have shaped the development of this package.</p> <ul> <li> <p>Constraint-Based Data Generation \u2013 Outlines Package: Willard, Brandon T. &amp; Louf, R\u00e9mi (2023). Efficient Guided Generation for LLMs. </p> </li> <li> <p>Chat Interface and Wrapper \u2013 Textual: McGugan, W. (2024, Sep). Anatomy of a Textual User Interface.</p> </li> <li> <p>Package Design Inspiration: Sloth, T. &amp; Rybner, A (2023). GendaLens </p> </li> <li> <p>Code Debugging and Conceptual Feedback:   Mina Almasi and Ross Deans Kristensen-McLachlan</p> </li> </ul>"},{"location":"installation/","title":"Installation","text":"<p>You can install EduChatEval using via pip from PyPI:</p> <p><code>pip install educhateval</code></p> <p>Or from Github:</p> <p><code>pip install git+https://github.com/laurawpaaby/EduChatEval.git</code></p>"},{"location":"api/api_frame_gen/","title":"Framework Generator","text":"<p>Module for generating synthetic annotated datasets (frameworks) using instruction-tuned models hosted locally and filtering of low-quality examples via classifier agreement.</p> <p>Attributes:</p> Name Type Description <code>model_name</code> <code>str</code> <p>Name of the local model to use for generation (default: \"llama-3.2-3b-instruct\").</p> <code>api_url</code> <code>str</code> <p>URL for the local model API (default: \"http://localhost:1234/v1/completions\").</p> <p>Methods:</p> Name Description <code>generate_framework</code> <p>Simulates a dialogue and returns it as a pandas DataFrame.</p> <code>filter_with_classifier</code> <p>Filters the generated dataset using a small classifier trained on real labeled data.</p> Source code in <code>src/educhateval/core.py</code> <pre><code>class FrameworkGenerator:\n    \"\"\"\n    Module for generating synthetic annotated datasets (frameworks) using instruction-tuned models hosted locally and filtering of low-quality examples via classifier agreement.\n\n    Attributes:\n        model_name (str): Name of the local model to use for generation (default: \"llama-3.2-3b-instruct\").\n        api_url (str): URL for the local model API (default: \"http://localhost:1234/v1/completions\").\n\n    Methods:\n        generate_framework(...): Simulates a dialogue and returns it as a pandas DataFrame.\n        filter_with_classifier(...): Filters the generated dataset using a small classifier trained on real labeled data.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_name: str = \"llama-3.2-3b-instruct\",\n        api_url: str = \"http://localhost:1234/v1/completions\",\n    ):\n        self.model_name = model_name\n        self.api_url = api_url\n\n    def generate_framework(\n        self,\n        prompt_path: str = None,\n        prompt_dict_input: dict = None,\n        num_samples: int = 500,\n        json_out: str = None,\n        csv_out: str = None,\n        seed: int = 42,\n        temperature: float = 0.85,\n        top_p: float = 0.90,\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Generate a synthetic labeled dataset from prompts using a language model.\n        Either `prompt_path` (path to .py file with `prompt_dict`) or `prompt_dict_input` must be provided.\n\n        Parameters:\n            prompt_path (str): Path to a Python file containing a prompt dictionary.\n            prompt_dict_input (dict): Prompt dictionary directly provided.\n            num_samples (int): Number of samples to generate per category.\n            json_out (str): Optional path to save JSON output.\n            csv_out (str): Optional path to save CSV output.\n            seed (int): Random seed for reproducibility.\n            temperature (float): Sampling temperature for generation.\n            top_p (float): Top-p sampling parameter.\n\n        Returns:\n            pd.DataFrame: Cleaned, labeled synthetic dataset.\n        \"\"\"\n        if not prompt_path and not prompt_dict_input:\n            raise ValueError(\n                \"You must provide either a prompt_path or prompt_dict_input.\"\n            )\n\n        set_seed(seed)\n\n        df = synthesize_dataset(\n            prompt_dict=prompt_dict_input,\n            prompt_path=prompt_path,\n            model_name=self.model_name,\n            num_samples=num_samples,\n            api_url=self.api_url,\n            json_out=json_out,\n            csv_out=csv_out,\n            temperature=temperature,\n            top_p=top_p,\n        )\n\n        return df\n\n    #### 2. function to quality check the dataset\n    def filter_with_classifier(\n        self,\n        train_data: Union[str, pd.DataFrame],\n        synth_data: Union[str, pd.DataFrame],\n        text_column: str = \"text\",\n        label_column: str = \"category\",\n        split_ratio: float = 0.2,\n        training_params: list = [0.01, \"cross_entropy\", 5e-5, 8, 8, 4, 0.01],\n        tuning: bool = False,\n        tuning_params: dict = None,\n        model_save_path: str = None,\n        classifier_model_name: str = \"distilbert-base-uncased\",\n        filtered_save_path: str = None,\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Train a small classifier on real labeled data and use it to filter the synthetic dataset by agreement.\n\n        Parameters:\n            train_data (str or pd.DataFrame): Path or DataFrame of small labeled training set.\n            synth_data (str or pd.DataFrame): Path or DataFrame of generated synthetic dataset.\n            text_column (str): Name of the text column.\n            label_column (str): Name of the label column.\n            split_ratio (float): Ratio for train/test split.\n            training_params (list): Training hyperparameters.\n            tuning (bool): Whether to perform hyperparameter tuning using Optuna.\n            tuning_params (dict): Optional tuning grid.\n            model_save_path (str): Optional path to save the classifier model.\n            classifier_model_name (str): HF model ID for the classifier.\n            filtered_save_path (str): Optional path to save filtered synthetic dataset.\n\n        Returns:\n            pd.DataFrame: Filtered synthetic dataset based on classifier agreement.\n        \"\"\"\n        if isinstance(train_data, pd.DataFrame) and train_data.empty:\n            raise ValueError(\"Provided training DataFrame is empty.\")\n        if isinstance(synth_data, pd.DataFrame) and synth_data.empty:\n            raise ValueError(\"Provided synthetic DataFrame is empty.\")\n\n        tokenizer = load_tokenizer(classifier_model_name)\n\n        dataset_dict, label2id = load_and_prepare_dataset(\n            train_data, text_column, label_column, split_ratio\n        )\n\n        tokenized = tokenize_dataset(dataset_dict, tokenizer)\n\n        model, trainer = train_model(\n            tokenized,\n            classifier_model_name,\n            len(label2id),\n            training_params,\n            tuning,\n            tuning_params,\n        )\n\n        trainer.evaluate()\n\n        if model_save_path:\n            save_model_and_tokenizer(model, tokenizer, model_save_path)\n\n        df_filtered = filter_synthesized_data(\n            synth_input=synth_data,\n            model=model,\n            tokenizer=tokenizer,\n            label_column=label_column,\n            save_path=filtered_save_path,\n        )\n\n        return df_filtered\n</code></pre>"},{"location":"api/api_frame_gen/#educhateval.core.FrameworkGenerator.generate_framework","title":"<code>generate_framework(prompt_path=None, prompt_dict_input=None, num_samples=500, json_out=None, csv_out=None, seed=42, temperature=0.85, top_p=0.9)</code>","text":"<p>Generate a synthetic labeled dataset from prompts using a language model. Either <code>prompt_path</code> (path to .py file with <code>prompt_dict</code>) or <code>prompt_dict_input</code> must be provided.</p> <p>Parameters:</p> Name Type Description Default <code>prompt_path</code> <code>str</code> <p>Path to a Python file containing a prompt dictionary.</p> <code>None</code> <code>prompt_dict_input</code> <code>dict</code> <p>Prompt dictionary directly provided.</p> <code>None</code> <code>num_samples</code> <code>int</code> <p>Number of samples to generate per category.</p> <code>500</code> <code>json_out</code> <code>str</code> <p>Optional path to save JSON output.</p> <code>None</code> <code>csv_out</code> <code>str</code> <p>Optional path to save CSV output.</p> <code>None</code> <code>seed</code> <code>int</code> <p>Random seed for reproducibility.</p> <code>42</code> <code>temperature</code> <code>float</code> <p>Sampling temperature for generation.</p> <code>0.85</code> <code>top_p</code> <code>float</code> <p>Top-p sampling parameter.</p> <code>0.9</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Cleaned, labeled synthetic dataset.</p> Source code in <code>src/educhateval/core.py</code> <pre><code>def generate_framework(\n    self,\n    prompt_path: str = None,\n    prompt_dict_input: dict = None,\n    num_samples: int = 500,\n    json_out: str = None,\n    csv_out: str = None,\n    seed: int = 42,\n    temperature: float = 0.85,\n    top_p: float = 0.90,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Generate a synthetic labeled dataset from prompts using a language model.\n    Either `prompt_path` (path to .py file with `prompt_dict`) or `prompt_dict_input` must be provided.\n\n    Parameters:\n        prompt_path (str): Path to a Python file containing a prompt dictionary.\n        prompt_dict_input (dict): Prompt dictionary directly provided.\n        num_samples (int): Number of samples to generate per category.\n        json_out (str): Optional path to save JSON output.\n        csv_out (str): Optional path to save CSV output.\n        seed (int): Random seed for reproducibility.\n        temperature (float): Sampling temperature for generation.\n        top_p (float): Top-p sampling parameter.\n\n    Returns:\n        pd.DataFrame: Cleaned, labeled synthetic dataset.\n    \"\"\"\n    if not prompt_path and not prompt_dict_input:\n        raise ValueError(\n            \"You must provide either a prompt_path or prompt_dict_input.\"\n        )\n\n    set_seed(seed)\n\n    df = synthesize_dataset(\n        prompt_dict=prompt_dict_input,\n        prompt_path=prompt_path,\n        model_name=self.model_name,\n        num_samples=num_samples,\n        api_url=self.api_url,\n        json_out=json_out,\n        csv_out=csv_out,\n        temperature=temperature,\n        top_p=top_p,\n    )\n\n    return df\n</code></pre>"},{"location":"api/api_frame_gen/#educhateval.core.FrameworkGenerator.filter_with_classifier","title":"<code>filter_with_classifier(train_data, synth_data, text_column='text', label_column='category', split_ratio=0.2, training_params=[0.01, 'cross_entropy', 5e-05, 8, 8, 4, 0.01], tuning=False, tuning_params=None, model_save_path=None, classifier_model_name='distilbert-base-uncased', filtered_save_path=None)</code>","text":"<p>Train a small classifier on real labeled data and use it to filter the synthetic dataset by agreement.</p> <p>Parameters:</p> Name Type Description Default <code>train_data</code> <code>str or DataFrame</code> <p>Path or DataFrame of small labeled training set.</p> required <code>synth_data</code> <code>str or DataFrame</code> <p>Path or DataFrame of generated synthetic dataset.</p> required <code>text_column</code> <code>str</code> <p>Name of the text column.</p> <code>'text'</code> <code>label_column</code> <code>str</code> <p>Name of the label column.</p> <code>'category'</code> <code>split_ratio</code> <code>float</code> <p>Ratio for train/test split.</p> <code>0.2</code> <code>training_params</code> <code>list</code> <p>Training hyperparameters.</p> <code>[0.01, 'cross_entropy', 5e-05, 8, 8, 4, 0.01]</code> <code>tuning</code> <code>bool</code> <p>Whether to perform hyperparameter tuning using Optuna.</p> <code>False</code> <code>tuning_params</code> <code>dict</code> <p>Optional tuning grid.</p> <code>None</code> <code>model_save_path</code> <code>str</code> <p>Optional path to save the classifier model.</p> <code>None</code> <code>classifier_model_name</code> <code>str</code> <p>HF model ID for the classifier.</p> <code>'distilbert-base-uncased'</code> <code>filtered_save_path</code> <code>str</code> <p>Optional path to save filtered synthetic dataset.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Filtered synthetic dataset based on classifier agreement.</p> Source code in <code>src/educhateval/core.py</code> <pre><code>def filter_with_classifier(\n    self,\n    train_data: Union[str, pd.DataFrame],\n    synth_data: Union[str, pd.DataFrame],\n    text_column: str = \"text\",\n    label_column: str = \"category\",\n    split_ratio: float = 0.2,\n    training_params: list = [0.01, \"cross_entropy\", 5e-5, 8, 8, 4, 0.01],\n    tuning: bool = False,\n    tuning_params: dict = None,\n    model_save_path: str = None,\n    classifier_model_name: str = \"distilbert-base-uncased\",\n    filtered_save_path: str = None,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Train a small classifier on real labeled data and use it to filter the synthetic dataset by agreement.\n\n    Parameters:\n        train_data (str or pd.DataFrame): Path or DataFrame of small labeled training set.\n        synth_data (str or pd.DataFrame): Path or DataFrame of generated synthetic dataset.\n        text_column (str): Name of the text column.\n        label_column (str): Name of the label column.\n        split_ratio (float): Ratio for train/test split.\n        training_params (list): Training hyperparameters.\n        tuning (bool): Whether to perform hyperparameter tuning using Optuna.\n        tuning_params (dict): Optional tuning grid.\n        model_save_path (str): Optional path to save the classifier model.\n        classifier_model_name (str): HF model ID for the classifier.\n        filtered_save_path (str): Optional path to save filtered synthetic dataset.\n\n    Returns:\n        pd.DataFrame: Filtered synthetic dataset based on classifier agreement.\n    \"\"\"\n    if isinstance(train_data, pd.DataFrame) and train_data.empty:\n        raise ValueError(\"Provided training DataFrame is empty.\")\n    if isinstance(synth_data, pd.DataFrame) and synth_data.empty:\n        raise ValueError(\"Provided synthetic DataFrame is empty.\")\n\n    tokenizer = load_tokenizer(classifier_model_name)\n\n    dataset_dict, label2id = load_and_prepare_dataset(\n        train_data, text_column, label_column, split_ratio\n    )\n\n    tokenized = tokenize_dataset(dataset_dict, tokenizer)\n\n    model, trainer = train_model(\n        tokenized,\n        classifier_model_name,\n        len(label2id),\n        training_params,\n        tuning,\n        tuning_params,\n    )\n\n    trainer.evaluate()\n\n    if model_save_path:\n        save_model_and_tokenizer(model, tokenizer, model_save_path)\n\n    df_filtered = filter_synthesized_data(\n        synth_input=synth_data,\n        model=model,\n        tokenizer=tokenizer,\n        label_column=label_column,\n        save_path=filtered_save_path,\n    )\n\n    return df_filtered\n</code></pre>"},{"location":"api/api_pred/","title":"Predicting Labels","text":"<p>Module for training and applying a text classification model.</p> <p>This class streamlines the process of fine-tuning a transformer-based classifier on labeled data and applying the trained model to annotate new, unlabeled datasets. Supports both single and multi-column predictions and includes optional model saving and evaluation output.</p> <p>Attributes:</p> Name Type Description <code>model_name</code> <code>str</code> <p>Name of the pretrained Hugging Face model to fine-tune (default: \"distilbert-base-uncased\").</p> <p>Methods:</p> Name Description <code>run_pipeline</code> <p>Trains the classifier and returns a DataFrame with predicted labels and confidence scores.</p> Source code in <code>src/educhateval/core.py</code> <pre><code>class PredictLabels:\n    \"\"\"\n    Module for training and applying a text classification model.\n\n    This class streamlines the process of fine-tuning a transformer-based classifier on labeled data\n    and applying the trained model to annotate new, unlabeled datasets. Supports both single and multi-column\n    predictions and includes optional model saving and evaluation output.\n\n    Attributes:\n        model_name (str): Name of the pretrained Hugging Face model to fine-tune (default: \"distilbert-base-uncased\").\n\n    Methods:\n        run_pipeline(...): Trains the classifier and returns a DataFrame with predicted labels and confidence scores.\n    \"\"\"\n\n    def __init__(self, model_name: str = \"distilbert-base-uncased\"):\n        self.model_name = model_name\n        self.tokenizer = load_tokenizer(model_name)\n\n    def run_pipeline(\n        self,\n        train_data: Union[str, pd.DataFrame],\n        new_data: Union[str, pd.DataFrame],\n        # columns in the training data\n        text_column: str = \"text\",\n        label_column: str = \"category\",\n        # columns to classify in the new data\n        columns_to_classify: Optional[Union[str, List[str]]] = None,\n        split_ratio: float = 0.2,\n        training_params: list = [0.01, \"cross_entropy\", 5e-5, 8, 8, 4, 0.01],\n        tuning: bool = False,\n        tuning_params: Optional[dict] = None,\n        model_save_path: Optional[str] = None,\n        prediction_save_path: Optional[str] = None,\n        seed: int = 42,\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        This function handles the full pipeline of loading data, preparing datasets, tokenizing inputs, training a transformer-based\n        classifier, and applying it to specified text columns in new data. It supports custom hyperparameters, optional hyperparameter\n        tuning, and saving of both the trained model and prediction outputs.\n\n        Parameters:\n            train_data (Union[str, pd.DataFrame]): Labeled dataset for training. Can be a DataFrame or a CSV file path.\n            new_data (Union[str, pd.DataFrame]): Dataset to annotate with predicted labels. Can be a DataFrame or a CSV file path.\n            text_column (str): Column in the training data containing the input text. Defaults to \"text\".\n            label_column (str): Column in the training data containing the target labels. Defaults to \"category\".\n            columns_to_classify (Optional[Union[str, List[str]]]): Column(s) in `new_data` to predict labels for. Defaults to `text_column`.\n            split_ratio (float): Ratio of data to use for validation. Must be between 0 and 1. Defaults to 0.2.\n            training_params (list): List of 7 training hyperparameters: [weight_decay, loss_fn, learning_rate, batch_size,\n                                num_epochs, warmup_steps, gradient_accumulation]. Defaults to [0.01, \"cross_entropy\", 5e-5, 8, 8, 4, 0.01].\n            tuning (bool): Whether to perform hyperparameter tuning. Defaults to False.\n            tuning_params (Optional[dict]): Dictionary of tuning settings if `tuning` is True. Defaults to None.\n            model_save_path (Optional[str]): Optional path to save the trained model and tokenizer. Defaults to None.\n            prediction_save_path (Optional[str]): Optional path to save annotated predictions as a CSV. Defaults to None.\n            seed (int): Random seed for reproducibility. Defaults to 42.\n\n        Returns:\n            pd.DataFrame: A DataFrame containing the original `new_data` with added columns for predicted labels and confidence scores.\n        \"\"\"\n\n        # Validate training data input\n        if not isinstance(train_data, (pd.DataFrame, str)):\n            raise ValueError(\n                \"Please provide data training data. This must be a pandas DataFrame or a path to a CSV file.\"\n            )\n\n        if not isinstance(new_data, (pd.DataFrame, str)):\n            raise ValueError(\n                \"Please provide data to be labeled. This must be a pandas DataFrame or a path to a CSV file.\"\n            )\n\n        # Validate training parameters\n        if not isinstance(training_params, list) or len(training_params) &lt; 7:\n            raise ValueError(\n                \"training_params must be a list of at least 7 hyperparameter values.\"\n            )\n\n        if not isinstance(split_ratio, float) or not (0.0 &lt; split_ratio &lt; 1.0):\n            raise ValueError(\"split_ratio must be a float between 0 and 1.\")\n\n        # Validate column names\n        if not isinstance(text_column, str):\n            raise ValueError(\"text_column must be a string.\")\n        if not isinstance(label_column, str):\n            raise ValueError(\"label_column must be a string.\")\n\n        # Validate columns_to_classify\n        if columns_to_classify is not None:\n            if not isinstance(columns_to_classify, (str, list)):\n                raise ValueError(\n                    \"columns_to_classify must be a string or a list of strings.\"\n                )\n            if isinstance(columns_to_classify, list) and not all(\n                isinstance(col, str) for col in columns_to_classify\n            ):\n                raise ValueError(\"All entries in columns_to_classify must be strings.\")\n\n        set_seed(seed)\n\n        dataset_dict, label2id = load_and_prepare_dataset(\n            train_data, text_column, label_column, split_ratio\n        )\n        tokenized = tokenize_dataset(dataset_dict, self.tokenizer)\n\n        model, trainer = train_model(\n            tokenized,\n            self.model_name,\n            len(label2id),\n            training_params,\n            tuning,\n            tuning_params,\n        )\n\n        if model_save_path:\n            save_model_and_tokenizer(model, self.tokenizer, model_save_path)\n\n        # Default to using the training text_column if no specific columns_to_classify provided\n        if columns_to_classify is None:\n            columns_to_classify = text_column\n\n        df_annotated = predict_annotated_dataset(\n            new_data=new_data,\n            model=model,\n            text_columns=columns_to_classify,\n            tokenizer=self.tokenizer,\n            label2id=label2id,\n            save_path=prediction_save_path,\n        )\n\n        return df_annotated\n</code></pre>"},{"location":"api/api_pred/#educhateval.core.PredictLabels.run_pipeline","title":"<code>run_pipeline(train_data, new_data, text_column='text', label_column='category', columns_to_classify=None, split_ratio=0.2, training_params=[0.01, 'cross_entropy', 5e-05, 8, 8, 4, 0.01], tuning=False, tuning_params=None, model_save_path=None, prediction_save_path=None, seed=42)</code>","text":"<p>This function handles the full pipeline of loading data, preparing datasets, tokenizing inputs, training a transformer-based classifier, and applying it to specified text columns in new data. It supports custom hyperparameters, optional hyperparameter tuning, and saving of both the trained model and prediction outputs.</p> <p>Parameters:</p> Name Type Description Default <code>train_data</code> <code>Union[str, DataFrame]</code> <p>Labeled dataset for training. Can be a DataFrame or a CSV file path.</p> required <code>new_data</code> <code>Union[str, DataFrame]</code> <p>Dataset to annotate with predicted labels. Can be a DataFrame or a CSV file path.</p> required <code>text_column</code> <code>str</code> <p>Column in the training data containing the input text. Defaults to \"text\".</p> <code>'text'</code> <code>label_column</code> <code>str</code> <p>Column in the training data containing the target labels. Defaults to \"category\".</p> <code>'category'</code> <code>columns_to_classify</code> <code>Optional[Union[str, List[str]]]</code> <p>Column(s) in <code>new_data</code> to predict labels for. Defaults to <code>text_column</code>.</p> <code>None</code> <code>split_ratio</code> <code>float</code> <p>Ratio of data to use for validation. Must be between 0 and 1. Defaults to 0.2.</p> <code>0.2</code> <code>training_params</code> <code>list</code> <p>List of 7 training hyperparameters: [weight_decay, loss_fn, learning_rate, batch_size,                 num_epochs, warmup_steps, gradient_accumulation]. Defaults to [0.01, \"cross_entropy\", 5e-5, 8, 8, 4, 0.01].</p> <code>[0.01, 'cross_entropy', 5e-05, 8, 8, 4, 0.01]</code> <code>tuning</code> <code>bool</code> <p>Whether to perform hyperparameter tuning. Defaults to False.</p> <code>False</code> <code>tuning_params</code> <code>Optional[dict]</code> <p>Dictionary of tuning settings if <code>tuning</code> is True. Defaults to None.</p> <code>None</code> <code>model_save_path</code> <code>Optional[str]</code> <p>Optional path to save the trained model and tokenizer. Defaults to None.</p> <code>None</code> <code>prediction_save_path</code> <code>Optional[str]</code> <p>Optional path to save annotated predictions as a CSV. Defaults to None.</p> <code>None</code> <code>seed</code> <code>int</code> <p>Random seed for reproducibility. Defaults to 42.</p> <code>42</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame containing the original <code>new_data</code> with added columns for predicted labels and confidence scores.</p> Source code in <code>src/educhateval/core.py</code> <pre><code>def run_pipeline(\n    self,\n    train_data: Union[str, pd.DataFrame],\n    new_data: Union[str, pd.DataFrame],\n    # columns in the training data\n    text_column: str = \"text\",\n    label_column: str = \"category\",\n    # columns to classify in the new data\n    columns_to_classify: Optional[Union[str, List[str]]] = None,\n    split_ratio: float = 0.2,\n    training_params: list = [0.01, \"cross_entropy\", 5e-5, 8, 8, 4, 0.01],\n    tuning: bool = False,\n    tuning_params: Optional[dict] = None,\n    model_save_path: Optional[str] = None,\n    prediction_save_path: Optional[str] = None,\n    seed: int = 42,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    This function handles the full pipeline of loading data, preparing datasets, tokenizing inputs, training a transformer-based\n    classifier, and applying it to specified text columns in new data. It supports custom hyperparameters, optional hyperparameter\n    tuning, and saving of both the trained model and prediction outputs.\n\n    Parameters:\n        train_data (Union[str, pd.DataFrame]): Labeled dataset for training. Can be a DataFrame or a CSV file path.\n        new_data (Union[str, pd.DataFrame]): Dataset to annotate with predicted labels. Can be a DataFrame or a CSV file path.\n        text_column (str): Column in the training data containing the input text. Defaults to \"text\".\n        label_column (str): Column in the training data containing the target labels. Defaults to \"category\".\n        columns_to_classify (Optional[Union[str, List[str]]]): Column(s) in `new_data` to predict labels for. Defaults to `text_column`.\n        split_ratio (float): Ratio of data to use for validation. Must be between 0 and 1. Defaults to 0.2.\n        training_params (list): List of 7 training hyperparameters: [weight_decay, loss_fn, learning_rate, batch_size,\n                            num_epochs, warmup_steps, gradient_accumulation]. Defaults to [0.01, \"cross_entropy\", 5e-5, 8, 8, 4, 0.01].\n        tuning (bool): Whether to perform hyperparameter tuning. Defaults to False.\n        tuning_params (Optional[dict]): Dictionary of tuning settings if `tuning` is True. Defaults to None.\n        model_save_path (Optional[str]): Optional path to save the trained model and tokenizer. Defaults to None.\n        prediction_save_path (Optional[str]): Optional path to save annotated predictions as a CSV. Defaults to None.\n        seed (int): Random seed for reproducibility. Defaults to 42.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing the original `new_data` with added columns for predicted labels and confidence scores.\n    \"\"\"\n\n    # Validate training data input\n    if not isinstance(train_data, (pd.DataFrame, str)):\n        raise ValueError(\n            \"Please provide data training data. This must be a pandas DataFrame or a path to a CSV file.\"\n        )\n\n    if not isinstance(new_data, (pd.DataFrame, str)):\n        raise ValueError(\n            \"Please provide data to be labeled. This must be a pandas DataFrame or a path to a CSV file.\"\n        )\n\n    # Validate training parameters\n    if not isinstance(training_params, list) or len(training_params) &lt; 7:\n        raise ValueError(\n            \"training_params must be a list of at least 7 hyperparameter values.\"\n        )\n\n    if not isinstance(split_ratio, float) or not (0.0 &lt; split_ratio &lt; 1.0):\n        raise ValueError(\"split_ratio must be a float between 0 and 1.\")\n\n    # Validate column names\n    if not isinstance(text_column, str):\n        raise ValueError(\"text_column must be a string.\")\n    if not isinstance(label_column, str):\n        raise ValueError(\"label_column must be a string.\")\n\n    # Validate columns_to_classify\n    if columns_to_classify is not None:\n        if not isinstance(columns_to_classify, (str, list)):\n            raise ValueError(\n                \"columns_to_classify must be a string or a list of strings.\"\n            )\n        if isinstance(columns_to_classify, list) and not all(\n            isinstance(col, str) for col in columns_to_classify\n        ):\n            raise ValueError(\"All entries in columns_to_classify must be strings.\")\n\n    set_seed(seed)\n\n    dataset_dict, label2id = load_and_prepare_dataset(\n        train_data, text_column, label_column, split_ratio\n    )\n    tokenized = tokenize_dataset(dataset_dict, self.tokenizer)\n\n    model, trainer = train_model(\n        tokenized,\n        self.model_name,\n        len(label2id),\n        training_params,\n        tuning,\n        tuning_params,\n    )\n\n    if model_save_path:\n        save_model_and_tokenizer(model, self.tokenizer, model_save_path)\n\n    # Default to using the training text_column if no specific columns_to_classify provided\n    if columns_to_classify is None:\n        columns_to_classify = text_column\n\n    df_annotated = predict_annotated_dataset(\n        new_data=new_data,\n        model=model,\n        text_columns=columns_to_classify,\n        tokenizer=self.tokenizer,\n        label2id=label2id,\n        save_path=prediction_save_path,\n    )\n\n    return df_annotated\n</code></pre>"},{"location":"api/api_synth_int/","title":"Synthesizing Interactions","text":"<p>Module for generating multi-turn dialogues between a student and tutor agent using large language models.</p> <p>This class wraps backend-specific model interfaces and orchestrates the simulation of conversations between two agents. It supports customizable educational modes and sampling behavior and ensures reproducibility via global seeding. Outputs are returned as structured pandas DataFrames.</p> <p>Attributes:</p> Name Type Description <code>backend</code> <code>str</code> <p>Backend to use for inference. Options are \"hf\" (Hugging Face) or \"mlx\" (MLX).</p> <code>model_id</code> <code>str</code> <p>The identifier of the model to use, e.g., \"gpt2\" (Hugging Face) or \"Qwen2.5-7B-Instruct-1M-4bit\" (MLX).</p> <code>sampling_params</code> <code>Optional[dict]</code> <p>Sampling hyperparameters such as temperature, top_p, or top_k.</p> <p>Methods:</p> Name Description <code>simulate_dialogue</code> <p>Simulates a dialogue and returns it as a pandas DataFrame.</p> Source code in <code>src/educhateval/core.py</code> <pre><code>class DialogueSimulator:\n    \"\"\"\n    Module for generating multi-turn dialogues between a student and tutor agent using large language models.\n\n    This class wraps backend-specific model interfaces and orchestrates the simulation of conversations between two agents.\n    It supports customizable educational modes and sampling behavior and ensures reproducibility via global seeding. Outputs are returned as structured pandas DataFrames.\n\n    Attributes:\n        backend (str): Backend to use for inference. Options are \"hf\" (Hugging Face) or \"mlx\" (MLX).\n        model_id (str): The identifier of the model to use, e.g., \"gpt2\" (Hugging Face) or \"Qwen2.5-7B-Instruct-1M-4bit\" (MLX).\n        sampling_params (Optional[dict]): Sampling hyperparameters such as temperature, top_p, or top_k.\n\n    Methods:\n        simulate_dialogue(...): Simulates a dialogue and returns it as a pandas DataFrame.\n    \"\"\"\n\n    def __init__(\n        self,\n        backend: str = \"mlx\",\n        model_id: str = \"mlx-community/Qwen2.5-7B-Instruct-1M-4bit\",\n        sampling_params: Optional[dict] = None,\n    ):\n        if backend == \"hf\":\n            self.model = ChatHF(\n                model_id=model_id,\n                sampling_params=sampling_params\n                or {\"temperature\": 0.9, \"top_p\": 0.9, \"top_k\": 50},\n            )\n        elif backend == \"mlx\":\n            self.model = ChatMLX(\n                model_id=model_id,\n                sampling_params=sampling_params\n                or {\"temp\": 0.9, \"top_p\": 0.9, \"top_k\": 40},\n            )\n        else:\n            raise ValueError(\"Unsupported backend\")\n\n        self.model.load()\n\n    def simulate_dialogue(\n        self,\n        mode: str = \"general_task_solving\",\n        turns: int = 5,\n        seed_message_input: str = \"Hi, I'm a student seeking assistance with my studies.\",\n        log_dir: Optional[Path] = None,\n        save_csv_path: Optional[Path] = None,\n        seed: int = 42,\n        custom_prompt_file: Optional[Path] = None,\n        system_prompts: Optional[dict] = None,\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Simulates a multi-turn dialogue using either built-in or custom prompts.\n\n        Args:\n            mode: Mode key to select prompt pair (student/tutor).\n            turns: Number of back-and-forth turns to simulate.\n            seed_message_input: First message from the student.\n            log_dir: Directory to save raw log (optional).\n            save_csv_path: Path to save structured DataFrame (optional).\n            seed: Random seed for reproducibility.\n            custom_prompt_file: Optional path to custom YAML defining prompt modes.\n            system_prompts: Optional dictionary of custom dict of prompt modes.\n\n        Returns:\n            pd.DataFrame: Structured DataFrame of the conversation.\n        \"\"\"\n        set_seed(seed)\n\n        # Validate input source\n        if system_prompts is not None and custom_prompt_file is not None:\n            raise ValueError(\"Provide only one of `system_prompts` or `custom_prompt_file`, not both.\")\n\n        # Load prompts from file if needed\n        if system_prompts is None:\n            if custom_prompt_file:\n                import yaml\n                try:\n                    with open(custom_prompt_file, \"r\") as f:\n                        custom_prompts = yaml.safe_load(f)\n                    print(f\" Loaded custom prompts from: {custom_prompt_file}\")\n                except Exception as e:\n                    raise ValueError(f\"Failed to load YAML from {custom_prompt_file}: {e}\")\n\n                if \"conversation_types\" not in custom_prompts:\n                    raise ValueError(f\"Missing 'conversation_types' in custom prompt file: {custom_prompt_file}\")\n\n                if mode not in custom_prompts[\"conversation_types\"]:\n                    raise ValueError(f\"Mode '{mode}' not found in custom prompt file: {custom_prompt_file}\")\n\n                system_prompts = custom_prompts[\"conversation_types\"][mode]\n\n            else:\n                # Use built-in fallback\n                print(\"Using default hardcoded prompts.\")\n                system_prompts = {\n                    \"student\": \"You are a student asking for help with a task.\",\n                    \"tutor\": \"You are a helpful tutor guiding the student step by step.\",\n                }\n\n        # Simulate conversation\n        df = simulate_conversation(\n            model=self.model,\n            turns=turns,\n            seed_message_input=seed_message_input,\n            log_dir=log_dir,\n            save_csv_path=save_csv_path,\n            system_prompts=system_prompts,\n            custom_prompt_file=None,  # already used, no need to pass again\n            mode=mode,\n        )\n\n        print(\"\\nFull dialogue stored in DataFrame. Use the returned object or view as `df`.\")\n        return df\n</code></pre>"},{"location":"api/api_synth_int/#educhateval.core.DialogueSimulator.simulate_dialogue","title":"<code>simulate_dialogue(mode='general_task_solving', turns=5, seed_message_input=\"Hi, I'm a student seeking assistance with my studies.\", log_dir=None, save_csv_path=None, seed=42, custom_prompt_file=None, system_prompts=None)</code>","text":"<p>Simulates a multi-turn dialogue using either built-in or custom prompts.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>str</code> <p>Mode key to select prompt pair (student/tutor).</p> <code>'general_task_solving'</code> <code>turns</code> <code>int</code> <p>Number of back-and-forth turns to simulate.</p> <code>5</code> <code>seed_message_input</code> <code>str</code> <p>First message from the student.</p> <code>\"Hi, I'm a student seeking assistance with my studies.\"</code> <code>log_dir</code> <code>Optional[Path]</code> <p>Directory to save raw log (optional).</p> <code>None</code> <code>save_csv_path</code> <code>Optional[Path]</code> <p>Path to save structured DataFrame (optional).</p> <code>None</code> <code>seed</code> <code>int</code> <p>Random seed for reproducibility.</p> <code>42</code> <code>custom_prompt_file</code> <code>Optional[Path]</code> <p>Optional path to custom YAML defining prompt modes.</p> <code>None</code> <code>system_prompts</code> <code>Optional[dict]</code> <p>Optional dictionary of custom dict of prompt modes.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Structured DataFrame of the conversation.</p> Source code in <code>src/educhateval/core.py</code> <pre><code>def simulate_dialogue(\n    self,\n    mode: str = \"general_task_solving\",\n    turns: int = 5,\n    seed_message_input: str = \"Hi, I'm a student seeking assistance with my studies.\",\n    log_dir: Optional[Path] = None,\n    save_csv_path: Optional[Path] = None,\n    seed: int = 42,\n    custom_prompt_file: Optional[Path] = None,\n    system_prompts: Optional[dict] = None,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Simulates a multi-turn dialogue using either built-in or custom prompts.\n\n    Args:\n        mode: Mode key to select prompt pair (student/tutor).\n        turns: Number of back-and-forth turns to simulate.\n        seed_message_input: First message from the student.\n        log_dir: Directory to save raw log (optional).\n        save_csv_path: Path to save structured DataFrame (optional).\n        seed: Random seed for reproducibility.\n        custom_prompt_file: Optional path to custom YAML defining prompt modes.\n        system_prompts: Optional dictionary of custom dict of prompt modes.\n\n    Returns:\n        pd.DataFrame: Structured DataFrame of the conversation.\n    \"\"\"\n    set_seed(seed)\n\n    # Validate input source\n    if system_prompts is not None and custom_prompt_file is not None:\n        raise ValueError(\"Provide only one of `system_prompts` or `custom_prompt_file`, not both.\")\n\n    # Load prompts from file if needed\n    if system_prompts is None:\n        if custom_prompt_file:\n            import yaml\n            try:\n                with open(custom_prompt_file, \"r\") as f:\n                    custom_prompts = yaml.safe_load(f)\n                print(f\" Loaded custom prompts from: {custom_prompt_file}\")\n            except Exception as e:\n                raise ValueError(f\"Failed to load YAML from {custom_prompt_file}: {e}\")\n\n            if \"conversation_types\" not in custom_prompts:\n                raise ValueError(f\"Missing 'conversation_types' in custom prompt file: {custom_prompt_file}\")\n\n            if mode not in custom_prompts[\"conversation_types\"]:\n                raise ValueError(f\"Mode '{mode}' not found in custom prompt file: {custom_prompt_file}\")\n\n            system_prompts = custom_prompts[\"conversation_types\"][mode]\n\n        else:\n            # Use built-in fallback\n            print(\"Using default hardcoded prompts.\")\n            system_prompts = {\n                \"student\": \"You are a student asking for help with a task.\",\n                \"tutor\": \"You are a helpful tutor guiding the student step by step.\",\n            }\n\n    # Simulate conversation\n    df = simulate_conversation(\n        model=self.model,\n        turns=turns,\n        seed_message_input=seed_message_input,\n        log_dir=log_dir,\n        save_csv_path=save_csv_path,\n        system_prompts=system_prompts,\n        custom_prompt_file=None,  # already used, no need to pass again\n        mode=mode,\n    )\n\n    print(\"\\nFull dialogue stored in DataFrame. Use the returned object or view as `df`.\")\n    return df\n</code></pre>"},{"location":"api/api_viz/","title":"Visualizer","text":"<p>Module for generating four different visualizations to analysis the interactions by.  Every visualization besides the 4. Interaction Distribution plot can be created for either the student, the tutor or both. The Interaction Distribution plot requires both student and tutor data to visualize interactions. </p> <p>1. Barchart of Predicted Classes </p> Source code in <code>src/educhateval/descriptive_results/display_results.py</code> <pre><code>def plot_category_bars(\n    df,\n    student_col=None,\n    tutor_col=None,\n    use_percent=True,\n    palette=\"icefire\",\n    title=\"Predicted Classes\",\n):\n    if not student_col and not tutor_col:\n        raise ValueError(\"You must provide at least one of student_col or tutor_col.\")\n\n    long_dfs = []\n    if student_col:\n        temp = df[[student_col]].copy()\n        temp[\"source\"] = \"Student\"\n        temp.rename(columns={student_col: \"predicted_label\"}, inplace=True)\n        long_dfs.append(temp)\n    if tutor_col:\n        temp = df[[tutor_col]].copy()\n        temp[\"source\"] = \"Tutor\"\n        temp.rename(columns={tutor_col: \"predicted_label\"}, inplace=True)\n        long_dfs.append(temp)\n\n    long_df = pd.concat(long_dfs, ignore_index=True)\n\n    all_labels = sorted(long_df[\"predicted_label\"].dropna().unique())\n    long_df[\"predicted_label\"] = pd.Categorical(\n        long_df[\"predicted_label\"], categories=all_labels, ordered=True\n    )\n\n    count_df = (\n        long_df.groupby([\"source\", \"predicted_label\"], observed=True)\n        .size()\n        .reset_index(name=\"count\")\n    )\n\n    if use_percent:\n        total_per_source = count_df.groupby(\"source\", observed=True)[\"count\"].transform(\n            \"sum\"\n        )\n        count_df[\"value\"] = (count_df[\"count\"] / total_per_source) * 100\n        y_label = \"Occurrences (%)\"\n        fmt = lambda val: f\"{val:.0f}%\"\n    else:\n        count_df[\"value\"] = count_df[\"count\"]\n        y_label = \"Number of Occurrences\"\n        fmt = lambda val: f\"{int(val)}\"\n\n    sns.set_style(\"whitegrid\")\n    plt.figure(figsize=(10, 6))\n\n    ax = sns.barplot(\n        data=count_df,\n        x=\"predicted_label\",\n        y=\"value\",\n        hue=\"source\",\n        palette=palette,\n        order=all_labels,\n    )\n\n    ax.set_xlabel(\"Predicted Category\")\n    ax.set_ylabel(y_label)\n    ax.set_title(title, fontsize=15, fontweight=\"bold\")\n\n    if use_percent:\n        ax.yaxis.set_major_formatter(mtick.FuncFormatter(lambda y, _: f\"{y:.0f}%\"))\n\n    for container in ax.containers:\n        for bar in container:\n            height = bar.get_height()\n            if height &gt; 0:\n                ax.annotate(\n                    fmt(height),\n                    xy=(bar.get_x() + bar.get_width() / 2, height),\n                    xytext=(0, 3),\n                    textcoords=\"offset points\",\n                    ha=\"center\",\n                    va=\"bottom\",\n                    fontsize=9,\n                )\n\n    plt.legend(title=\"Agent\")\n    plt.tight_layout()\n    plt.show()\n</code></pre> <p>Example usage:  <pre><code>viz.plot_category_bars(\n    df=annotated_df,\n    student_col=\"predicted_labels_student_msg\",\n    tutor_col=\"predicted_labels_tutor_msg\",\n    use_percent=True,\n    title=\"Distribution of Predicted Classes\"\n)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> The input DataFrame containing predicted categories for student and/or tutor. required <code>student_col</code> <code>str</code> or <code>None</code> Name of the column with student-predicted labels. Optional. <code>None</code> <code>tutor_col</code> <code>str</code> or <code>None</code> Name of the column with tutor-predicted labels. Optional. <code>None</code> <code>palette</code> <code>str</code> Color palette used for the plot. Optional. <code>\"icefire\"</code> <code>title</code> <code>str</code> Title of the plot. Optional. <code>\"Predicted Classes\"</code> <p>Returns:</p> Name Type Description \u2014 <code>None</code> Displays the plot using <code>matplotlib.pyplot.show()</code>. No object is returned. <p>2. Summary Table </p> Source code in <code>src/educhateval/descriptive_results/display_results.py</code> <pre><code>def create_prediction_summary_table(df, student_col=None, tutor_col=None):\n    if not student_col and not tutor_col:\n        raise ValueError(\"You must provide at least one of student_col or tutor_col.\")\n\n    result_dfs = []\n    all_categories = set()\n\n    if student_col:\n        student_counts = df[student_col].value_counts(dropna=False)\n        total = student_counts.sum()\n        counts = student_counts.rename(\"Student (n)\")\n        percents = ((student_counts / total) * 100).round(1).astype(str) + \"%\"\n        percents.name = \"Student (%)\"\n        merged = pd.concat([counts, percents], axis=1)\n        result_dfs.append(merged)\n        all_categories.update(merged.index)\n\n    if tutor_col:\n        tutor_counts = df[tutor_col].value_counts(dropna=False)\n        total = tutor_counts.sum()\n        counts = tutor_counts.rename(\"Tutor (n)\")\n        percents = ((tutor_counts / total) * 100).round(1).astype(str) + \"%\"\n        percents.name = \"Tutor (%)\"\n        merged = pd.concat([counts, percents], axis=1)\n        result_dfs.append(merged)\n        all_categories.update(merged.index)\n\n    full_index = pd.Index(sorted(all_categories), name=\"Predicted Category\")\n    summary_df = pd.DataFrame(index=full_index)\n\n    for df_part in result_dfs:\n        summary_df = summary_df.join(df_part, how=\"left\")\n\n    for col in summary_df.columns:\n        if \"(n)\" in col:\n            summary_df[col] = summary_df[col].fillna(0).astype(int)\n        elif \"(%)\" in col:\n            summary_df[col] = summary_df[col].fillna(\"0.0%\")\n\n    summary_df = summary_df.reset_index()\n    return summary_df\n</code></pre> <p>Example usage:  <pre><code>summary = viz.create_summary_table(\n    df=annotated_df,\n    student_col=\"predicted_labels_student_msg\",\n    tutor_col=\"predicted_labels_tutor_msg\"\n)\n\nprint(summary)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> The input DataFrame containing predicted categories for student and/or tutor. required <code>student_col</code> <code>str</code> or <code>None</code> Name of the column with student-predicted labels. Optional. <code>None</code> <code>tutor_col</code> <code>str</code> or <code>None</code> Name of the column with tutor-predicted labels. Optional. <code>None</code> <p>Returns:</p> Name Type Description <code>summary_df</code> <code>DataFrame</code> A summary table with counts and percentages for each predicted category. Splits by student and tutor (if provided). Missing values are filled with 0. <p>3. Predicted Classes by Turns </p> Source code in <code>src/educhateval/descriptive_results/display_results.py</code> <pre><code>def plot_predicted_categories(\n    df,\n    student_col=None,\n    tutor_col=None,\n    use_percent=True,\n    palette=\"icefire\",\n    title=\"Predicted Category Distribution\",\n    show_ci=False,\n):\n    if not student_col and not tutor_col:\n        raise ValueError(\"You must provide at least one of student_col or tutor_col.\")\n\n    # Prepare long format\n    long_dfs = []\n    if student_col:\n        temp = df[[\"turn\", student_col]].copy()\n        temp[\"source\"] = \"Student\"\n        temp.rename(columns={student_col: \"predicted_label\"}, inplace=True)\n        long_dfs.append(temp)\n    if tutor_col:\n        temp = df[[\"turn\", tutor_col]].copy()\n        temp[\"source\"] = \"Tutor\"\n        temp.rename(columns={tutor_col: \"predicted_label\"}, inplace=True)\n        long_dfs.append(temp)\n\n    long_df = pd.concat(long_dfs, ignore_index=True)\n\n    all_labels = sorted(long_df[\"predicted_label\"].dropna().unique())\n    long_df[\"predicted_label\"] = pd.Categorical(\n        long_df[\"predicted_label\"], categories=all_labels, ordered=True\n    )\n\n    count_df = (\n        long_df.groupby([\"turn\", \"source\", \"predicted_label\"], observed=True)\n        .size()\n        .reset_index(name=\"count\")\n    )\n\n    if use_percent:\n        total_per_group = count_df.groupby([\"turn\", \"source\"], observed=True)[\n            \"count\"\n        ].transform(\"sum\")\n        count_df[\"value\"] = (count_df[\"count\"] / total_per_group) * 100\n        y_label = \"Occurrences (%)\"\n        fmt = lambda y, _: f\"{y:.0f}%\"\n        y_max = 100\n    else:\n        count_df[\"value\"] = count_df[\"count\"]\n        y_label = \"Number of Occurrences\"\n        fmt = lambda y, _: f\"{int(y)}\"\n        y_max = count_df[\"value\"].max() + 3\n\n    sns.set_style(\"whitegrid\")\n    g = sns.relplot(\n        data=count_df,\n        x=\"turn\",\n        y=\"value\",\n        hue=\"predicted_label\",\n        kind=\"line\",\n        col=\"source\" if student_col and tutor_col else None,\n        facet_kws={\"sharey\": True, \"sharex\": True},\n        height=4.5,\n        aspect=1.5,\n        marker=\"o\",\n        palette=palette,\n        hue_order=all_labels,\n        errorbar=('ci', 95) if show_ci else None,\n    )\n\n    if student_col and tutor_col:\n        g.set_titles(\"{col_name} Messages\")\n    g.set_axis_labels(\"Turn\", y_label)\n\n    g.fig.subplots_adjust(right=0.85)\n    g._legend.set_bbox_to_anchor((1.12, 0.5))\n    g._legend.set_frame_on(True)\n    g._legend.set_title(\"Predicted Category\")\n\n    for ax in g.axes.flat:\n        ax.set_ylim(0, y_max)\n        ax.yaxis.set_major_formatter(mtick.FuncFormatter(fmt))\n\n    plt.suptitle(title, fontsize=15, fontweight=\"bold\", y=0.95)\n    plt.tight_layout()\n    plt.show()\n</code></pre> <p>Example usage:  <pre><code>plot_predicted_categories(\n        df=annotated_df,\n        student_col=\"predicted_labels_student_msg\",\n        tutor_col=\"predicted_labels_tutor_msg\",\n        title=\"Predicted Category Distribution\"\n    )\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> Input DataFrame with turn-level predicted labels for student and/or tutor. required <code>student_col</code> <code>str</code> or <code>None</code> Name of the column containing student-predicted categories. Optional. <code>None</code> <code>tutor_col</code> <code>str</code> or <code>None</code> Name of the column containing tutor-predicted categories. Optional. <code>None</code> <code>use_percent</code> <code>bool</code> Whether to plot percentage values (<code>True</code>) or raw counts (<code>False</code>). <code>True</code> <code>palette</code> <code>str</code> Color palette used for the plot. Optional. <code>\"icefire\"</code> <code>title</code> <code>str</code> Title of the plot. Optional. <code>\"Predicted Classes\"</code> <p>Returns:</p> Name Type Description \u2014 <code>None</code> Displays the plot using <code>matplotlib.pyplot.show()</code>. No object is returned. <p>4. Interaction Distribution </p> <p>Plot the distribution of predicted categories in the previous turn of the opposite agent. Both student and tutor is required.</p> Source code in <code>src/educhateval/descriptive_results/display_results.py</code> <pre><code>def plot_previous_turn_distribution(\n    df,\n    student_col=\"predicted_labels_student_msg\",\n    tutor_col=\"predicted_labels_tutor_msg\",\n    focus_agent=\"student\",\n    use_percent=True,\n    palette=\"icefire\",\n    title=None,\n):\n    \"\"\"\n    Plot the distribution of predicted categories in the previous turn of the *opposite* agent. Both student and tutor is required.\n    \"\"\"\n\n    if not student_col or not tutor_col:\n        raise ValueError(\"Both student_col and tutor_col must be provided.\")\n\n    if focus_agent not in [\"student\", \"tutor\"]:\n        raise ValueError(\"focus_agent must be either 'student' or 'tutor'.\")\n\n    if focus_agent == \"student\":\n        if not student_col or not tutor_col:\n            raise ValueError(\n                \"Both student_col and tutor_col must be provided when focus_agent='student'.\"\n            )\n        focus_col = student_col\n        opposite_col = tutor_col\n        focus_label = \"Student\"\n        opposite_label = \"Tutor\"\n    else:\n        if not student_col or not tutor_col:\n            raise ValueError(\n                \"Both student_col and tutor_col must be provided when focus_agent='tutor'.\"\n            )\n        focus_col = tutor_col\n        opposite_col = student_col\n        focus_label = \"Tutor\"\n        opposite_label = \"Student\"\n\n    # Prepare shifted column\n    df_sorted = df.sort_values(by=[\"student_id\", \"turn\"]).copy()\n    df_sorted[\"prev_opposite_label\"] = df_sorted.groupby(\"student_id\")[\n        opposite_col\n    ].shift(1)\n    df_filtered = df_sorted.dropna(subset=[focus_col, \"prev_opposite_label\"])\n\n    # Count combinations\n    grouped = (\n        df_filtered.groupby([focus_col, \"prev_opposite_label\"], observed=True)\n        .size()\n        .reset_index(name=\"count\")\n    )\n\n    if use_percent:\n        total_per_focus = grouped.groupby(focus_col, observed=True)[\"count\"].transform(\n            \"sum\"\n        )\n        grouped[\"percentage\"] = (grouped[\"count\"] / total_per_focus) * 100\n        y_col = \"percentage\"\n        y_label = f\"Category in Previous Turn for {opposite_label} (%)\"\n        fmt = lambda val: f\"{val:.0f}%\"\n    else:\n        grouped[\"percentage\"] = grouped[\"count\"]\n        y_col = \"count\"\n        y_label = f\"Category in Previous Turn for {opposite_label} (n)\"\n        fmt = lambda val: f\"{int(val)}\"\n\n    # Ensure all category combinations are represented\n    focus_vals = sorted(df_filtered[focus_col].dropna().unique())\n    prev_vals = sorted(df_filtered[\"prev_opposite_label\"].dropna().unique())\n    full_grid = pd.MultiIndex.from_product(\n        [focus_vals, prev_vals], names=[focus_col, \"prev_opposite_label\"]\n    ).to_frame(index=False)\n    grouped = full_grid.merge(\n        grouped, on=[focus_col, \"prev_opposite_label\"], how=\"left\"\n    ).fillna(0)\n    grouped[\"count\"] = grouped[\"count\"].astype(int)\n    if use_percent:\n        grouped[\"percentage\"] = (\n            grouped.groupby(focus_col)[\"count\"]\n            .transform(lambda x: x / x.sum() * 100)\n            .fillna(0)\n        )\n\n    grouped = grouped.sort_values(by=[focus_col, \"prev_opposite_label\"])\n\n    # Plot\n    sns.set_style(\"whitegrid\")\n    g = sns.catplot(\n        data=grouped,\n        x=focus_col,\n        y=y_col,\n        hue=\"prev_opposite_label\",\n        kind=\"bar\",\n        palette=palette,\n        height=6,\n        aspect=2.5,\n        dodge=True,\n        order=focus_vals,\n        hue_order=prev_vals,\n    )\n\n    # Adjust bar width\n    for patch in g.ax.patches:\n        patch.set_width(patch.get_width() * 0.9)\n\n    # Labels and title\n    g.set_axis_labels(f\"Category in Current Turn for {focus_label}\", y_label)\n    g.fig.suptitle(\n        f\"Distribution of Interactions: {focus_label} Focus\",\n        fontsize=15,\n        fontweight=\"bold\",\n        y=0.99,\n    )\n\n    if use_percent:\n        g.ax.set_ylim(0, 100)\n        g.ax.yaxis.set_major_formatter(mtick.FuncFormatter(lambda y, _: f\"{y:.0f}%\"))\n\n    # Annotate values (including 0s)\n    dodge_width = 0.8 / len(prev_vals)\n    for i, row in grouped.iterrows():\n        x_pos = focus_vals.index(row[focus_col])\n        hue_idx = prev_vals.index(row[\"prev_opposite_label\"])\n        xpos_shifted = x_pos - 0.4 + dodge_width / 2 + hue_idx * dodge_width\n        height = row[y_col]\n        g.ax.annotate(\n            fmt(height),\n            xy=(xpos_shifted, height),\n            xytext=(0, 3),\n            textcoords=\"offset points\",\n            ha=\"center\",\n            va=\"bottom\",\n            fontsize=9,\n        )\n\n    g.fig.subplots_adjust(right=0.85)\n    g._legend.set_bbox_to_anchor((1.12, 0.5))\n    g._legend.set_frame_on(True)\n    g._legend.set_title(f\"{opposite_label} Category (Turn - 1)\")\n\n\n    plt.suptitle(title, fontsize=15, fontweight=\"bold\", y=0.95)\n\n    plt.tight_layout()\n    plt.show()\n</code></pre> <p>Example usage: <pre><code>viz.plot_history_interaction(\n    df=annotated_df,\n    student_col=\"predicted_labels_student_msg\",\n    tutor_col=\"predicted_labels_tutor_msg\",\n    focus_agent=\"tutor\",\n    use_percent=True\n)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> Input DataFrame including turn-level predicted labels for both student and tutor. required <code>student_col</code> <code>str</code> or <code>None</code> Column name containing student-predicted categories. required <code>tutor_col</code> <code>str</code> or <code>None</code> Column name containing tutor-predicted categories. required <code>focus_agent</code> <code>str</code> Determines whether to analyze the student or tutor perspective. Options: \"student\" or \"tutor\". <code>\"student\"</code> <code>use_percent</code> <code>bool</code> If <code>True</code>, the y-axis will display percentages; otherwise raw counts are shown. <code>True</code> <code>palette</code> <code>str</code> Color palette used for the plot. Optional. <code>\"icefire\"</code> <p>Returns:</p> Name Type Description \u2014 <code>None</code> Displays the plot using <code>matplotlib.pyplot.show()</code>. No object is returned."},{"location":"api/api_wrap_int/","title":"Wrap Interactions","text":"<p>To initiate and wrap a dialogue between a user and LLM-tutor locally, follow this tutorial. </p> <p>Module for launching the Textual chat interface with an LM Studio-backed language model. It should be run from the command line.</p> <p>This class configures the model, system prompt, and save path, and then starts an interactive, full-screen terminal chat application using the <code>textual</code> framework. The interface supports live message streaming, styled user and assistant blocks, and automatic logging of chat history to JSON and CSV formats.</p> <p>Parameters:</p> Name Type Description Default <code>api_url</code> <code>str</code> <p>URL of the LM Studio API endpoint used for inference. Defaults to \"http://127.0.0.1:1234/v1/chat/completions\".</p> <code>'http://127.0.0.1:1234/v1/chat/completions'</code> <code>model_name</code> <code>str</code> <p>Name of the model to use in LM Studio (e.g., \"llama-3.2-3b-instruct\").</p> <code>'llama-3.2-3b-instruct'</code> <code>temperature</code> <code>float</code> <p>Sampling temperature for text generation. Controls randomness. Defaults to 0.7.</p> <code>0.7</code> <code>system_prompt</code> <code>str</code> <p>Initial system message to prime the assistant\u2019s behavior. Defaults to a helpful tutoring message.</p> <code>'You are a helpful tutor guiding a student. Answer short and concisely.'</code> <code>save_dir</code> <code>Path</code> <p>Directory for saving logged conversations as <code>.json</code> and <code>.csv</code>. Defaults to \"data/logged_dialogue_data\".</p> <code>Path('data/logged_dialogue_data')</code> <p>Methods:</p> Name Description <code>run</code> <p>Launches the Textual chat application and starts interaction with the model.</p> Source code in <code>src/educhateval/chat_ui.py</code> <pre><code>class ChatWrap:\n    \"\"\"\n    Module for launching the Textual chat interface with an LM Studio-backed language model. It should be run from the command line.\n\n    This class configures the model, system prompt, and save path, and then starts an interactive, full-screen terminal chat\n    application using the `textual` framework. The interface supports live message streaming, styled user and assistant blocks,\n    and automatic logging of chat history to JSON and CSV formats.\n\n    Parameters:\n        api_url (str): URL of the LM Studio API endpoint used for inference. Defaults to \"http://127.0.0.1:1234/v1/chat/completions\".\n        model_name (str): Name of the model to use in LM Studio (e.g., \"llama-3.2-3b-instruct\").\n        temperature (float): Sampling temperature for text generation. Controls randomness. Defaults to 0.7.\n        system_prompt (str): Initial system message to prime the assistant\u2019s behavior. Defaults to a helpful tutoring message.\n        save_dir (Path): Directory for saving logged conversations as `.json` and `.csv`. Defaults to \"data/logged_dialogue_data\".\n\n    Methods:\n        run(): Launches the Textual chat application and starts interaction with the model.\n\n\n    \"\"\"\n\n    def __init__(\n        self,\n        api_url: str = \"http://127.0.0.1:1234/v1/chat/completions\",\n        model_name: str = \"llama-3.2-3b-instruct\",\n        temperature: float = 0.7,\n        system_prompt: str = \"You are a helpful tutor guiding a student. Answer short and concisely.\",\n        save_dir: Path = Path(\"data/logged_dialogue_data\"),\n    ):\n        self.api_url = api_url\n        self.model_name = model_name\n        self.temperature = temperature\n        self.system_prompt = system_prompt\n        self.save_dir = save_dir\n\n        # Initialize model and conversation history\n        self.model = ChatLMStudio(\n            api_url=self.api_url,\n            model_name=self.model_name,\n            temperature=self.temperature,\n        )\n        self.chat_history = ChatHistory(\n            messages=[ChatMessage(role=\"system\", content=self.system_prompt)]\n        )\n\n    def run(self):\n        \"\"\"Launches the Textual app.\"\"\"\n        app = ChatApp(\n            model=self.model,\n            chat_history=self.chat_history,\n            chat_messages_dir=self.save_dir,\n        )\n        app.run()\n</code></pre> <p>Example Usage from Terminal: <pre><code>chat-ui \\\n  --api_url http://127.0.0.1:1234/v1/chat/completions \\\n  --model llama-3.2-3b-instruct \\\n  --prompt \"You are a helpful tutor guiding a student.\" \\\n  --save_dir data/logged_dialogue_data\n</code></pre></p>"},{"location":"api/api_wrap_int/#educhateval.chat_ui.ChatWrap.run","title":"<code>run()</code>","text":"<p>Launches the Textual app.</p> Source code in <code>src/educhateval/chat_ui.py</code> <pre><code>def run(self):\n    \"\"\"Launches the Textual app.\"\"\"\n    app = ChatApp(\n        model=self.model,\n        chat_history=self.chat_history,\n        chat_messages_dir=self.save_dir,\n    )\n    app.run()\n</code></pre>"},{"location":"user_guides/frameworks/","title":"Templates","text":"<p>Several modules within the pipeline accept customized prompts and seed messages to ensure that the synthesized data aligns with the specific interactional setting the user intends to simulate. Below are templates of both prompts and seed provided to illustrate how such customization can guide the generation process and support targeted use cases.</p>"},{"location":"user_guides/frameworks/#feedback-templates","title":"Feedback Templates","text":"<p>This section demonstrates a pipeline configuration centered on feedback types, inspired by the work of Hansen et al. 2025. The template framework focuses on the following categories: Advice, Content, Encouragement, Explanation, Posed Question, Small Talk, Specificity, and Style. </p>"},{"location":"user_guides/frameworks/#framework-generation","title":"Framework Generation","text":"<p>The goal of this setup is to expose the language model to targeted examples of interaction types it is expected to simulate. Each prompt encourages the model to produce sentences belonging to a specific category. </p> <p>For the generation of a framework for the eight categories (Advice, Content, Encouragement, Explanation, Posed Question, Small Talk, Specificity, and Style), the model was provided with the template presented below. </p> Prompt Template for Feedback: YAML <pre><code>prompt_dict = {\n    # ADVICE\n    \"Advice\": \"\"\"&lt;|im_start|&gt;system\nYou provide a single sentence of advice in an educational setting.\nThe advice should be constructive, context-appropriate, and expressed in a helpful tone. You never repeat yourself.\n&lt;|im_end|&gt;\n\n&lt;|im_start|&gt;user\nGive a sentence of advice.\n&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\"\"\",\n\n    # CONTENT\n    \"Content\": \"\"\"&lt;|im_start|&gt;system\nYou state an academic fact or piece of content relevant to a student's learning.\nEach sentence should be informative, domain-relevant, and suited for tutoring. You never repeat yourself.\n&lt;|im_end|&gt;\n\n&lt;|im_start|&gt;user\nProvide a factual statement.\n&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\"\"\",\n\n    # ENCOURAGEMENT\n    \"Encouragement\": \"\"\"&lt;|im_start|&gt;system\nYou generate a single sentence offering encouragement to a student.\nThe tone should be warm, supportive, and appropriate for educational feedback. You never repeat yourself.\n&lt;|im_end|&gt;\n\n&lt;|im_start|&gt;user\nSay something encouraging.\n&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\"\"\",\n\n    # EXPLANATION\n    \"Explanation\": \"\"\"&lt;|im_start|&gt;system\nYou provide a one-sentence explanation of a concept or process relevant in education.\nYour explanations should be clear, concise, and accurate. You never repeat yourself.\n&lt;|im_end|&gt;\n\n&lt;|im_start|&gt;user\nExplain a concept in one sentence.\n&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\"\"\",\n\n    # POSED QUESTION\n    \"Posed Question\": \"\"\"&lt;|im_start|&gt;system\nYou pose an open-ended or reflective question to stimulate learning.\nThe questions should be clear, relevant, and suitable for a tutor to ask a student. You never repeat the same question.\n&lt;|im_end|&gt;\n\n&lt;|im_start|&gt;user\nPose a thoughtful question.\n&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\"\"\",\n\n    # SMALL TALK\n    \"Small Talk\": \"\"\"&lt;|im_start|&gt;system\nYou generate short, friendly small talk suitable for casual rapport between student and tutor.\nKeep it relaxed and appropriate for an educational setting. You never repeat yourself.\n&lt;|im_end|&gt;\n\n&lt;|im_start|&gt;user\nSay something casual.\n&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\"\"\",\n\n\n# SPECIFICITY\n\"Specificity\": \"\"\"&lt;|im_start|&gt;system\nYou generate a sentence that highlights a specific part of the user\u2019s message by referencing it directly.\nThis may include precise feedback, correction, or specific praise. Be educational and avoid repetition.\n&lt;|im_end|&gt;\n\n&lt;|im_start|&gt;user\nHere is my sentence: I has always enjoyed to learn new things.\n&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\nThe phrase \u201cI has always enjoyed\u201d should be corrected to \u201cI have always enjoyed\u201d to match subject-verb agreement.\n&lt;|im_end|&gt;\n&lt;|im_start|&gt;user\nI think my main argument was strong.\n&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\nYes, the statement \u201cmy main argument was strong\u201d is supported well by the example you gave about online learning.\n&lt;|im_end|&gt;\n&lt;|im_start|&gt;user\nGive a new specific comment on a new sentence.\n&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\"\"\",\n\n    # STYLE\n    \"Style\": \"\"\"&lt;|im_start|&gt;system\nYou highlight a stylistic feature in the user's sentence. This can include vivid language, varied sentence structure, tone shifts, or rhetorical devices.\nMake sure your response is educational, concise, and never repeats itself. Do not rephrase the user's sentence or generate a new one.\n&lt;|im_end|&gt;\n\n&lt;|im_start|&gt;user\nThe thunder cracked like a whip, and the sky spilled open in a furious storm.\n&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\nYour sentence uses vivid imagery and a simile (\u201ccracked like a whip\u201d) to create a dramatic, sensory-rich description.\n&lt;|im_end|&gt;\n&lt;|im_start|&gt;user\nHer words, soft as feathers, floated across the room and lingered in the silence.\n&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\nThis sentence demonstrates strong style through its gentle simile and poetic rhythm, which evoke a calm, reflective mood.\n&lt;|im_end|&gt;\n&lt;|im_start|&gt;user\nGive another sentence and highlight a stylistic feature.\n&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\"\"\"\n\n}\n</code></pre>"},{"location":"user_guides/frameworks/#synthesizing-interactions","title":"Synthesizing Interactions","text":"<p>To generate synthetic dialogues, the interaction generator relies on system prompts for both the student and tutor agents. These prompts must be structured under a top-level key named <code>conversation_types</code>, whether passed as a <code>dictionary</code> or a <code>YAML</code> file. This format allows multiple interaction modes to be stored in a single configuration. If no prompting scheme is provided, a simple default prompt is used.</p> <p>Below are examples of the expected format in both a <code>dictionary</code> and a <code>YAML</code> file. The <code>YAML</code> template demonstrates how to store multiple frameworks, in this example for both feedback and linguistic categories.</p> <p>Dictionary Template: <pre><code># a dictionary of system prompts\ncustom_prompts = {\n    \"conversation_types\": {\n        \"general_task_solving\": {\n            \"student\": \"You are a student asking for help with a task.\",\n            \"tutor\": \"You are a helpful tutor guiding the student step by step.\"\n        },\n    }\n}\n\nsystem_prompts = custom_prompts[\"conversation_types\"][\"general_task_solving\"]\n</code></pre></p> <p>YAML Template: <pre><code>conversation_types:\n\n  general_task_solving:\n    student: &gt;\n      You are an AI pretending to be a student trying to solve a task of your own choice.\n\n      Your instructions are:\n      - Ask for clarification, examples, or deeper explanations until you are confident you understand everything.\n      - When asking questions, only ask one at a time.\n      - Only ask simple questions relevant to the task. No broad or vague questions.\n      - Never teach.\n      - Always ask for short and consice answers.\n    tutor: &gt;\n      You are an AI tutor for a student trying to solve a task.\n\n      Your instructions are:\n      - Answer clearly and consicely.\n      - Check for understanding.\n      - Always be short and concise.\n\n  evaluative_feedback:\n    student: &gt;\n      You are an AI pretending to be a student submitting some work and looking for constructive feedback.\n\n      Your instructions are:\n      - Present your work clearly and concisely.\n      - Ask for feedback on the work.\n      - When asking questions, only ask one at a time.\n      - Always ask for short and consice answers.\n\n      After receiving feedback, ask follow-up questions or offer revised versions for further improvement.\n    tutor: &gt;\n      You are a an AI tutor providing feedback on student work.\n\n      Your instructions are:\n      - Always be short and concise.\n      - Feedback should be constructive and specific.\n      - Feedback should be varied and not repetitive in both content and style.\n      - Invite the student to reflect or revise their work.\n</code></pre></p> <p>In addition to prompts, the interaction generator requires a seed message to initiate the dialogue. This is the first message sent by the student-agent. Seed messages can be passed directly as strings or organized in a <code>YAML</code> file when generating multiple dialogues. The example below illustrates both formats, based on an feedback context.</p> <p>Example of single seed: <pre><code>seed_msg = \"Hi, I'm a student seeking feedback on my work, please provide me useful and precise feedback.\"\n</code></pre></p> <p>Example of multiple seeds in YAML: <pre><code>feedback_session:\n  seeds:\n    - \"Hi, can you give me feedback on my essay introduction?\"\n    - \"Hello, I just submitted a paragraph\u2014what do you think I could improve?\"\n    - \"Hey, I'd like feedback on the structure of my report. Is it clear?\"\n    - \"Hi, could you point out any grammar issues in my writing?\"\n    - \"Hi, I\u2019m trying to make my argument stronger\u2014can you help me refine it?\"\n</code></pre></p>"},{"location":"user_guides/frameworks/#linguistic-templates","title":"Linguistic Templates","text":"<p>This section demonstrates a pipeline configuration centered on simple linguistic interaction types, inspired by the communicative categories proposed by Wei et al. (2022). The template framework focuses on the following categories: Clarification, Question, Small Talk, and Statement. These categories reflect the communicative intent behind user inputs\u2014whether from the student or the LLM-driven tutor.</p> <p>While Clarification, Question, and Small Talk are directly derived from the study, the Statement category has been added to capture direct, informative responses that serve an answering function. It reflects utterances that do not seek new information but instead contribute content to the dialogue, as frequently observed in educational contexts.</p>"},{"location":"user_guides/frameworks/#framework-generation_1","title":"Framework Generation","text":"<p>The goal of this setup is to expose the language model to targeted examples of interaction types it is expected to simulate. Each prompt encourages the model to produce sentences belonging to a specific category. Notably, only the Statement category involves full question\u2013answer pairs, ensuring that responses align with the discourse patterns typically found in learning environments.</p> <p>For the generation of a framework for the four categories (Clarification, Question, Small Talk and Statement), the model was provided with the prompt template presented below. </p> Prompt Template for Four Linguistic Categories: YAML <pre><code>prompt_dict = {\n    # CLARIFICATION\n    \"Clarification\": \"\"\"&lt;|im_start|&gt;system\nYou generate a conversational sentence that seeks clarification or context. \nIt should be polite, concise, and appropriate for an educational setting. You never repeat yourself.\n&lt;|im_end|&gt;\n\n&lt;|im_start|&gt;user\nCreate a clarification sentence.\n&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\nCould you explain what you meant by the term 'distributed cognition'?\n&lt;|im_end|&gt;\n&lt;|im_start|&gt;user\nCreate another clarification sentence.\n&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\nPlease provide more details about what the assignment is asking for?    \n&lt;|im_end|&gt;\n&lt;|im_start|&gt;user\nCreate a clarification sentence.\n&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\"\"\",\n\n    # SMALL TALK\n    \"Small Talk\": \"\"\"&lt;|im_start|&gt;system\nYou generate a short small talk sentence suitable in an casual setting. You never repeat yourself.\n&lt;|im_end|&gt;\n\n&lt;|im_start|&gt;user\nCreate a small talk sentence.\n&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\nHi, how are you?\n&lt;|im_end|&gt;\n&lt;|im_start|&gt;user\nCreate another small talk sentence.\n&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\"\"\",\n\n    # QUESTION\n    \"Question\": \"\"\"&lt;|im_start|&gt;system\nYou generate a factual or thoughtful question that can be used in a conversation or educational setting. You never repeat the same question.\n&lt;|im_end|&gt;\n\n&lt;|im_start|&gt;user\nCreate a question.\n&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\nWhat is the capital of France?\n&lt;|im_end|&gt;\n&lt;|im_start|&gt;user\nCreate a question.\n&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\nWhat is 24 times 25?\n&lt;|im_end|&gt;\n&lt;|im_start|&gt;user\nCreate a question.\n&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\"\"\",\n\n    # STATEMENT\n    \"Statement\": \"\"\"&lt;|im_start|&gt;system\nYou are a helpful and knowledgeable assistant in an educational setting.\nYou respond to student questions in a friendly and conversational tone, aiming to explain or clarify in one sentence.\nEach response should:\n- Address the question naturally, like a tutor or teacher would.\n- Stick to one main idea or explanation per response.\n- Avoid repeating previous answers or stating obvious facts. \n- Don't write the question you are answering.\n&lt;|im_end|&gt;\n\n&lt;|im_start|&gt;user\nWhy do we need sleep?\n&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\nSleep helps your brain and body recover and process everything you\u2019ve learned during the day.\n&lt;|im_end|&gt;\n&lt;|im_start|&gt;user\nwhat is the translation of hi how are you doing in portugese?\n&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\nThe translation of 'Hi, how are you doing?\" in Portuguese is: \"Oi, como voc\u00ea est\u00e1?'\n&lt;|im_end|&gt;\n&lt;|im_start|&gt;user\nGive a new, unique one-sentence answer to a new and different question.\n&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\"\"\",\n}\n</code></pre>"},{"location":"user_guides/frameworks/#synthesizing-interactions_1","title":"Synthesizing Interactions","text":"<p>To generate synthetic dialogues, the interaction generator relies on system prompts for both the student and tutor agents. These prompts must be structured under a top-level key named <code>conversation_types</code>, whether passed as a <code>dictionary</code> or a <code>YAML</code> file. This format allows multiple interaction modes to be stored in a single configuration. If no prompting scheme is provided, a simple default prompt is used.</p> <p>Below are examples of the expected format in both a <code>dictionary</code> and a <code>YAML</code> file. The <code>YAML</code> template demonstrates how to store multiple frameworks, such as feedback-oriented and linguistically structured interactions.</p> <p>Dictionary Template: <pre><code># a dictionary of system prompts\ncustom_prompts = {\n    \"conversation_types\": {\n        \"general_task_solving\": {\n            \"student\": \"You are a student asking for help with a task.\",\n            \"tutor\": \"You are a helpful tutor guiding the student step by step.\"\n        },\n    }\n}\n\nsystem_prompts = custom_prompts[\"conversation_types\"][\"general_task_solving\"]\n</code></pre></p> <p>YAML Scheme: The yaml template for this task are the same as used in the synthesization of feedback data.</p> <p>As mentioned above, the interaction generator requires a seed message to initiate the dialogue. The example below illustrates both formats, based on an English course context.</p> <p>Example of single seed: <pre><code>seed_msg = \"Hi, I'm a student seeking assistance with my studies in English. I need precise and easy-to-understand answers.\"\n</code></pre></p> <p>Example of multiple seeds in YAML: <pre><code>english_course:\n  seeds:\n    - \"Hi, I'm struggling with writing essays. Can you help me improve?\"\n    - \"Hi, Can you explain how to use the passive voice in English?\"\n    - \"Hi, I'm preparing for an English exam\u2014what are common grammar mistakes to watch out for?\"\n    - \"Hi, How can I expand my vocabulary effectively?\"\n    - \"Hi, Can you help me analyze this short story for class?\"\n</code></pre></p>"},{"location":"user_guides/frameworks/#wrap-interaction","title":"Wrap Interaction","text":"<p>The chat wrapper is not used in the provided guides. However, example prompts for initiating a local dialogue with the LLM-based tutor are provided below.</p> <p>To use the wrapper for local student\u2013tutor interaction, follow the Chat Wrap Tutorial.</p>"},{"location":"user_guides/frameworks/#example-prompts","title":"Example Prompts","text":"<pre><code>--prompt \"You are a helpful tutor guiding a student.\"\n# or\n--prompt \"You are a helpful tutor providing feedback on inputs by the student.\"\n# or\n--prompt \"You are a helpful tutor helping a student translate Spanish to English. Make sure to break it down.\"\n</code></pre>"},{"location":"user_guides/frameworks/#references","title":"References","text":"<ul> <li>Hansen, R., Prilop, C. N., Alsted Nielsen, T., M\u00f8ller, K. L., Fr\u00f8hlich Hougaard, R., &amp; B\u00fcchert Lindberg, A. (2025). The effects of an AI feedback coach on students\u2019 peer feedback quality, composition, and feedback experience. Tidsskriftet L\u00e6ring Og Medier (LOM), 17(31). https://doi.org/10.7146/lom.v17i31.148831</li> <li>Wei, Y., Lu, W., Cheng, Q., Jiang, T., &amp; Liu, S. (2022). How humans obtain information from AI: Categorizing user messages in human-AI collaborative conversations. Information Processing &amp; Management, 59(2), 102838. https://doi.org/10.1016/j.ipm.2021.102838</li> </ul>"},{"location":"user_guides/guide/","title":"Guides","text":"<p>Below are two example workflows demonstrating how to use each module in the package to synthesize, classify, and visualize dialogue interactions between a student and tutor agent. There are two use cases: first for a feedback focused scenario, the second for a intention focused scenario.  In practice, users are encouraged to incorporate real-world data where available, which may slightly alter certain steps in the pipeline.</p>"},{"location":"user_guides/guide/#feedback-example","title":"Feedback Example","text":"<p>Generate Label Framework</p> <p>The first example walks through the full pipeline for generating, classifying, and visualizing dialogue interactions focused on different feedback types: (Advice, Content, Encouragement, Explanation, Posed Question, Small Talk, Specificity, and Style). </p> <p>First install the package: <pre><code>pip install educhateval\n</code></pre> Initialize the generator with your chosen model and API, and generate labeled samples: <pre><code>generator = FrameworkGenerator(model_name=\"llama-3.2-3b-instruct\", api_url=\"http://localhost:1234/v1/completions\")\n\n# Generating the raw data \ndf_feedback = generator.generate_framework(\n    prompt_path=\"/outline_prompts/prompt_feedback.py\", \n    num_samples=200, \n    csv_out=\"/data/labeled_feedback_data.csv\"\n)\n\n# Filtering based on classifier agreement to ensure high-quality framework\nfiltered_df = generator.filter_with_classifier(\n    train_data=\"data/tiny_labeled_feedback.csv\", \n    synth_data=df_feedback,\n    classifier_model_name = \"distilbert-base-uncased\"\n)\n</code></pre></p> <p>Synthesize Interaction</p> <p>Import and initialize the dialogue simulator:</p> <pre><code>from educhateval import DialogueSimulator\nsimulator = DialogueSimulator(backend=\"mlx\", model_id=\"mlx-community/Qwen2.5-7B-Instruct-1M-4bit\")\n</code></pre> <p>In this example a system prompt and seed message is given directly, see Templates for how to structure a <code>YAML</code> input instead.  <pre><code># making the custom prompt as dict\ncustom_prompts = {\n    \"conversation_types\": {\n        \"feedback\": {\n            \"student\": \"You are a student asking for feedback on your work.\",\n            \"tutor\": \"You are a helpful tutor providing focused, costructive feedback.\"\n        },\n    }\n}\n\n# setting prompt and seed msg\nprompt = custom_prompts[\"conversation_types\"][\"feedback\"]\nseed_message = \"I've written this paragraph, please help me polish it: flowers are nice and they grow in gardens. some are red or yellow. people give them on birthdays and stuff. bees like them too.\" \n</code></pre></p> <p>In this case the interactions of only one student is simulated. For an example where several student-tutor interactions are simulated and appended to a <code>DataFrame</code> look at the intention scenario. </p> <pre><code>df_single_feedback = simulator.simulate_dialogue(\n    mode=\"feedback\",\n    turns=10,\n    seed_message_input=seed_message\n    system_prompts=prompt\n)\n</code></pre> <p>Classify and Predict</p> <p>Import the classifier and run prediction on the labeled, synthesized data. In this case only the <code>tutor messages</code> are of interest, as it is the feedback types provided by the llm that are being analyzed. <pre><code>from educhateval import PredictLabels\n\npredictor = PredictLabels(model_name=\"distilbert/distilroberta-base\")\n\n# Run the full prediction pipeline\nannotaded_feedback = predictor.run_pipeline(\n    train_data=filtered_df, \n    new_data=df_single_feedback, \n    text_column=\"text\",\n    label_column=\"category\", \n    columns_to_classify=[\"tutor_msg\"], \n    split_ratio=0.25\n)\n</code></pre></p> <p>Visualize</p> <p>Import the visualizer and plot results: <pre><code>from educhateval import Visualizer\n\nviz = Visualizer()\n</code></pre></p> <p>Generate plots - here focusing on the bar charts: <pre><code># Bar plot of class distributions\nviz.plot_category_bars(\n    df=annotaded_feedback,\n    label_columns=[\"predicted_labels_student_msg\", \"predicted_labels_tutor_msg\"],\n    use_percent=True,\n    title=\"Distribution of Predicted Feedback Classes\",\n    palette=\"twilight\"\n)\n</code></pre></p> <p>Returns:</p> <p>{align=center, width=\"300\"}</p> <p>{align=center, width=\"300\"}</p> <p><pre><code># Trend plot of predicted categories over turns\nviz.plot_turn_trends(\n    df=annotaded_feedback,\n    student_col=\"predicted_labels_student_msg\",\n    tutor_col=\"predicted_labels_tutor_msg\",\n    title=\"Feedback Distribution over Turns\",\n    show_ci=True\n)\n</code></pre> Returns:</p>"},{"location":"user_guides/guide/#intention-example","title":"Intention Example","text":"<p>The second example walks through the full pipeline for generating, classifying, and visualizing dialogue interactions focused on intentions behind messages: (Clarification, Question, Small Talk and Statement).</p> <p>First install the package: <pre><code>pip install educhateval\n</code></pre></p> <p>Generate Label Framework</p> <p>Import the framework generator: <pre><code>from educhateval import FrameworkGenerator\n</code></pre></p> <p>Initialize the generator with your chosen model and API, and generate labeled samples. In this example a dictionary is provided as prompt.  See Templates for how to structure a <code>YAML</code> prompt input instead. </p> <pre><code>generator = FrameworkGenerator(model_name=\"llama-3.2-3b-instruct\", api_url=\"http://localhost:1234/v1/completions\")\n\n# Dictionary of prompts\ncustom_prompt_dict = {\n        # CLARIFICATION\n        \"Clarification\": \"\"\"&lt;|im_start|&gt;system\n    You generate a conversational sentence that seeks clarification or context. \n    It should be polite, concise, and appropriate for an educational setting. You never repeat yourself.\n    &lt;|im_end|&gt;\n\n    &lt;|im_start|&gt;user\n    Create a clarification sentence.\n    &lt;|im_end|&gt;\n    &lt;|im_start|&gt;assistant\"\"\",\n\n        # SMALL TALK\n        \"Small Talk\": \"\"\"&lt;|im_start|&gt;system\n    You generate a short small talk sentence suitable in an casual setting. You never repeat yourself.\n    &lt;|im_end|&gt;\n\n    &lt;|im_start|&gt;user\n    Create a small talk sentence.\n    &lt;|im_end|&gt;\n    &lt;|im_start|&gt;assistant\"\"\",\n\n        # QUESTION\n        \"Question\": \"\"\"&lt;|im_start|&gt;system\n    You generate a factual or thoughtful question that can be used in a conversation or educational setting. You never repeat the same question.\n    &lt;|im_end|&gt;\n\n    &lt;|im_start|&gt;user\n    Create a question.\n    &lt;|im_end|&gt;\n    &lt;|im_start|&gt;assistant\"\"\",\n\n        # STATEMENT\n        \"Statement\": \"\"\"&lt;|im_start|&gt;system\n    You are a helpful and knowledgeable assistant in an educational setting.\n    You respond to student questions in a friendly and conversational tone, aiming to explain or clarify in one sentence.\n    Each response should:\n    - Address the question naturally, like a tutor or teacher would.\n    - Stick to one main idea or explanation per response.\n    - Avoid repeating previous answers or stating obvious facts. \n    - Don't write the question you are answering.\n    &lt;|im_end|&gt;\n\n    &lt;|im_start|&gt;user\n    Give a unique one-sentence answer to a new and different question.\n    &lt;|im_end|&gt;\n    &lt;|im_start|&gt;assistant\"\"\",\n    }\n</code></pre> <p>Now using the dictionary directly: <pre><code># Generating the raw data \ndf_4 = generator.generate_framework(\n    prompt_path=custom_prompt_dict,\n    num_samples=200, \n    csv_out=\"/data/labeled_training_data.csv\"\n)\n\n# Filtering based on classifier agreement to ensure high-quality framework\nfiltered_df = generator.filter_with_classifier(\n    train_data=\"data/tiny_labeled_default.csv\", \n    synth_data=df_4,\n    classifier_model_name = \"distilbert-base-uncased\"\n)\n</code></pre></p> <p>Synthesize Interaction</p> <p>Import and initialize the dialogue simulator: <pre><code>from educhateval import DialogueSimulator\n\nsimulator = DialogueSimulator(backend=\"mlx\", model_id=\"mlx-community/Qwen2.5-7B-Instruct-1M-4bit\")\n</code></pre> In this example, multiple dialogues are simulated representing different student agents. Each seed message in the <code>english_course</code> configuration acts as the starting input for one student. The model responds over several turns, alternating between the student and tutor roles. This looped setup helps create a diverse set of interaction sequences, each grounded in a unique initiating prompt.</p> <p>See Templates for how to structure your <code>YAML</code> seed input used here. </p> <pre><code># Extract seed messages for the English course\nenglish_seeds = seed_messages[\"english_course\"][\"seeds\"]\nn_seeds = len(english_seeds)\n\n# Store all simulated dialogues\nenglish_dialogues = []\n\nfor i, seed_message in enumerate(english_seeds):\n    df_single = simulator.simulate_dialogue(\n        mode=\"general_task_solving\",\n        turns=8,\n        seed_message_input=seed_message,\n        custom_prompt_file=Path(\"prompts/my_custom_prompts.yaml\")\n    )\n\n    # Add metadata\n    df_single[\"student_id\"] = f\"english_course_student_{i+1}\"\n    df_single[\"course\"] = \"english_course\"\n    english_dialogues.append(df_single)\n\n# Combine all dialogues\nenglish_course_df = pd.concat(english_dialogues, ignore_index=True)\n</code></pre> <p>Classify and Predict</p> <p>Import the classifier and run prediction on your labeled and synthesized data: <pre><code>from educhateval import PredictLabels\n\npredictor = PredictLabels(model_name=\"distilbert/distilroberta-base\")\n\n# Run the full prediction pipeline\nannotaded_df = predictor.run_pipeline(\n    train_data=filtered_df, \n    new_data=english_course_df, \n    text_column=\"text\",\n    label_column=\"category\", \n    columns_to_classify=[\"student_msg\", \"tutor_msg\"], \n    split_ratio=0.20\n)\n</code></pre></p> <p>Visualize Import the visualizer and plot results: <pre><code>from educhateval import Visualizer\n\nviz = Visualizer()\n</code></pre></p> <p>Generate summary tables and plots: <pre><code># Summary table of predicted categories\nsummary = viz.create_summary_table(\n    df=annotaded_df, \n    student_col=\"predicted_labels_student_msg\",\n    tutor_col=\"predicted_labels_tutor_msg\"\n)\nprint(summary)\n\n# Bar plot of class distributions\nviz.plot_category_bars(\n    df=annotaded_df,\n    student_col=\"predicted_labels_student_msg\",\n    tutor_col=\"predicted_labels_tutor_msg\",\n    use_percent=True,\n    title=\"Frequency of Predicted Classes\"\n)\n</code></pre></p> <p>Returns:</p> <pre><code># Trend plot of predicted categories over turns\nviz.plot_turn_trends(\n    df=annotated_df,\n    student_col=\"predicted_labels_student_msg\",\n    tutor_col=\"predicted_labels_tutor_msg\",\n    title=\"Category Frequencies over Turns\",\n    show_ci=False,\n)\n</code></pre> <p>Returns:</p> <p>Lastly the tutor\u2013student interaction patterns can be visualized as sequential category dependencies, here with a student focus. The x-axis represents the student's current input category, while each bar shows the distribution of tutor response types from the previous turn. This is possible when both student and tutor inputs are predicted and labeled. </p> <p><pre><code># Dependency plot\nviz.plot_history_interaction(\n    df=annotated_df,\n    student_col=\"predicted_labels_student_msg\",\n    tutor_col=\"predicted_labels_tutor_msg\",\n    focus_agent=\"student\",\n    use_percent=True\n)\n</code></pre> Returns:</p>"},{"location":"user_guides/guide/#direct-wrap-of-interactions","title":"Direct Wrap of Interactions","text":"<p>Instead of generating or using pre-existing interaction data, users can collect it directly using the package\u2019s <code>chat_ui</code> wrapper.  his replaces the synthesizing interactions step above and is executed as follows:</p> <pre><code>pip install educhateval\n</code></pre> <p>From the terminal run: </p> <pre><code>chat-ui \\\n  --api_url http://127.0.0.1:1234/v1/chat/completions \\\n  --model llama-3.2-3b-instruct \\\n  --prompt \"You are a helpful tutor guiding a student.\" \\\n  --save_dir data/logged_dialogue_data\n</code></pre> <p>Opens: </p>  ![ui](../pics/ui.png)  <p>Find more detailed instructions in the Chat Wrap Tutorial.</p>"},{"location":"user_guides/userguide_intro/","title":"Overview","text":"<p>This user guide presents two illustrative use cases that demonstrate the functionality and flexibility of the package, along with a set of prompt and seed templates tailored for different application settings. While no real-world data was available at the time of development, the examples are designed to showcase the pipeline's potential through simulated scenarios.</p> <p>The first use case centers on feedback generation, analyzing the types of feedback produced by a tutor agent in response to diverse student tasks. The second scenario shifts focus to modeling message intentions in multi-turn dialogues between student and tutor agents.</p> <p>Detailed information on the available classes, functions, and configuration options can be found in the API reference.</p> <p>Continue to Guides or see Templates</p>"}]}