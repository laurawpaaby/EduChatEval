{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Lauras Python Package","text":"<p>If your interested in evaluating the usage of bots in education yay </p>"},{"location":"about/","title":"About","text":""},{"location":"about/#project","title":"Project","text":"<p>This package was developed as part of a master\u2019s thesis on the MSc in Cognitive Science at Aarhus University. With this package, I hope to contribute to the development of methods for evaluating the use of LLMs.</p> <p>The main contribution is refining and wrapping existing methods into a coherent test suite. Additionally, we have introduced our own evaluation and visualization functions to present the results of various metrics in a cohesive manner.</p>"},{"location":"about/#contact","title":"Contact","text":"<p>If you have any questions regarding the project or the code implementation, feel free to contact someone@example.com.</p> <p>Chatwrap is licensed under MIT and available on GitHub.</p>"},{"location":"about/#acknowledgements","title":"Acknowledgements","text":"<p>This project uses code from three already implemented frameworks for quantifying gender bias in Danish. While all externally authored code is properly attributed in the repository\u2019s scripts, we would also like to acknowledge the authors of the work we draw on:</p> <p>Reference 1: Author, A. A. (Year). Title of the article. Journal Title, 10(2), 123\u2013145. https://doi.org/10.0000/xyz123 Reference 2: Author, B. B., &amp; Author, C. C. (Year). Another work. Publisher. https://doi.org/10.0000/abc456 Reference 3: Author, D. D. (Year). Yet another reference. Conference Proceedings. https://doi.org/10.0000/def789 Reference 4: Organization E. (Year). Document title. Retrieved from https://doi.org/10.0000/ghi012</p>"},{"location":"installation/","title":"Installation","text":"<p>You can install chat wrappy using via pip from PyPI:</p> <p><code>pip install chat-wrappy</code></p> <p>Or from Github:</p> <p><code>pip install git+https://github.com/laurawpaaby/EduChatEval.git</code></p>"},{"location":"api/api_frame_gen/","title":"Framework Generator","text":"<p>This is a test displaying: </p> <p>High-level interface for generating synthetic datasets (frameworks) using prompts and local LLMs.</p> <p>Includes: - Synthetic data generation using instruction-tuned models. - Filtering of low-quality examples via classifier agreement with a small labeled dataset.</p> Source code in <code>src/educhateval/core.py</code> <pre><code>class FrameworkGenerator:\n    \"\"\"\n    High-level interface for generating synthetic datasets (frameworks) using prompts and local LLMs.\n\n    Includes:\n    - Synthetic data generation using instruction-tuned models.\n    - Filtering of low-quality examples via classifier agreement with a small labeled dataset.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_name: str = \"llama-3.2-3b-instruct\",\n        api_url: str = \"http://localhost:1234/v1/completions\",\n    ):\n        \"\"\"\n        Initialize the framework generator.\n\n        Parameters:\n        - model_name (str): Name of the generative model to use (e.g., llama-3.2-3b-instruct).\n        - api_url (str): URL endpoint to communicate with the locally hosted LLM (e.g., LM Studio).\n        \"\"\"\n        self.model_name = model_name\n        self.api_url = api_url\n\n    def generate_framework(\n        self,\n        prompt_path: str = None,\n        prompt_dict_input: dict = None,\n        num_samples: int = 500,\n        json_out: str = None,\n        csv_out: str = None,\n        seed: int = 42,\n        temperature: float = 0.85,\n        top_p: float = 0.90,\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Generate a synthetic labeled dataset from prompts using a language model.\n\n        Either `prompt_path` (path to .py file with `prompt_dict`) or `prompt_dict_input` must be provided.\n\n        Parameters:\n        - prompt_path (str): Path to a Python file containing a prompt dictionary.\n        - prompt_dict_input (dict): Prompt dictionary directly provided.\n        - num_samples (int): Number of samples to generate per category.\n        - json_out (str): Optional path to save JSON output.\n        - csv_out (str): Optional path to save CSV output.\n        - seed (int): Random seed for reproducibility.\n\n        Returns:\n        - pd.DataFrame: Cleaned, labeled synthetic dataset.\n        \"\"\"\n        if not prompt_path and not prompt_dict_input:\n            raise ValueError(\n                \"You must provide either a prompt_path or prompt_dict_input.\"\n            )\n\n        set_seed(seed)\n\n        df = synthesize_dataset(\n            prompt_dict=prompt_dict_input,\n            prompt_path=prompt_path,\n            model_name=self.model_name,\n            num_samples=num_samples,\n            api_url=self.api_url,\n            json_out=json_out,\n            csv_out=csv_out,\n            temperature=temperature,\n            top_p=top_p,\n        )\n\n        return df\n\n    #### 2. function to quality check the dataset\n    def filter_with_classifier(\n        self,\n        train_data: Union[str, pd.DataFrame],\n        synth_data: Union[str, pd.DataFrame],\n        text_column: str = \"text\",\n        label_column: str = \"category\",\n        split_ratio: float = 0.2,\n        training_params: list = [0.01, \"cross_entropy\", 5e-5, 8, 8, 4, 0.01],\n        tuning: bool = False,\n        tuning_params: dict = None,\n        model_save_path: str = None,\n        classifier_model_name: str = \"distilbert-base-uncased\",\n        filtered_save_path: str = None,\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Train a small classifier on real labeled data and use it to filter the synthetic dataset by agreement.\n\n        Parameters:\n        - train_data (str or pd.DataFrame): Path or DataFrame of small labeled training set.\n        - synth_data (str or pd.DataFrame): Path or DataFrame of generated synthetic dataset.\n        - text_column (str): Name of the text column.\n        - label_column (str): Name of the label column.\n        - split_ratio (float): Ratio for train/test split.\n        - training_params (list): Training hyperparameters.\n        - tuning (bool): Whether to perform hyperparameter tuning using Optuna.\n        - tuning_params (dict): Optional tuning grid.\n        - model_save_path (str): Optional path to save the classifier model.\n        - classifier_model_name (str): HF model ID for the classifier.\n        - filtered_save_path (str): Optional path to save filtered synthetic dataset.\n\n        Returns:\n        - pd.DataFrame: Filtered synthetic dataset based on classifier agreement.\n        \"\"\"\n        if isinstance(train_data, pd.DataFrame) and train_data.empty:\n            raise ValueError(\"Provided training DataFrame is empty.\")\n        if isinstance(synth_data, pd.DataFrame) and synth_data.empty:\n            raise ValueError(\"Provided synthetic DataFrame is empty.\")\n\n        tokenizer = load_tokenizer(classifier_model_name)\n\n        dataset_dict, label2id = load_and_prepare_dataset(\n            train_data, text_column, label_column, split_ratio\n        )\n\n        tokenized = tokenize_dataset(dataset_dict, tokenizer)\n\n        model, trainer = train_model(\n            tokenized,\n            classifier_model_name,\n            len(label2id),\n            training_params,\n            tuning,\n            tuning_params,\n        )\n\n        trainer.evaluate()\n\n        if model_save_path:\n            save_model_and_tokenizer(model, tokenizer, model_save_path)\n\n        df_filtered = filter_synthesized_data(\n            synth_input=synth_data,\n            model=model,\n            tokenizer=tokenizer,\n            label_column=label_column,\n            save_path=filtered_save_path,\n        )\n\n        return df_filtered\n</code></pre>"},{"location":"api/api_frame_gen/#educhateval.core.FrameworkGenerator.__init__","title":"<code>__init__(model_name='llama-3.2-3b-instruct', api_url='http://localhost:1234/v1/completions')</code>","text":"<p>Initialize the framework generator.</p> <p>Parameters: - model_name (str): Name of the generative model to use (e.g., llama-3.2-3b-instruct). - api_url (str): URL endpoint to communicate with the locally hosted LLM (e.g., LM Studio).</p> Source code in <code>src/educhateval/core.py</code> <pre><code>def __init__(\n    self,\n    model_name: str = \"llama-3.2-3b-instruct\",\n    api_url: str = \"http://localhost:1234/v1/completions\",\n):\n    \"\"\"\n    Initialize the framework generator.\n\n    Parameters:\n    - model_name (str): Name of the generative model to use (e.g., llama-3.2-3b-instruct).\n    - api_url (str): URL endpoint to communicate with the locally hosted LLM (e.g., LM Studio).\n    \"\"\"\n    self.model_name = model_name\n    self.api_url = api_url\n</code></pre>"},{"location":"api/api_frame_gen/#educhateval.core.FrameworkGenerator.filter_with_classifier","title":"<code>filter_with_classifier(train_data, synth_data, text_column='text', label_column='category', split_ratio=0.2, training_params=[0.01, 'cross_entropy', 5e-05, 8, 8, 4, 0.01], tuning=False, tuning_params=None, model_save_path=None, classifier_model_name='distilbert-base-uncased', filtered_save_path=None)</code>","text":"<p>Train a small classifier on real labeled data and use it to filter the synthetic dataset by agreement.</p> <p>Parameters: - train_data (str or pd.DataFrame): Path or DataFrame of small labeled training set. - synth_data (str or pd.DataFrame): Path or DataFrame of generated synthetic dataset. - text_column (str): Name of the text column. - label_column (str): Name of the label column. - split_ratio (float): Ratio for train/test split. - training_params (list): Training hyperparameters. - tuning (bool): Whether to perform hyperparameter tuning using Optuna. - tuning_params (dict): Optional tuning grid. - model_save_path (str): Optional path to save the classifier model. - classifier_model_name (str): HF model ID for the classifier. - filtered_save_path (str): Optional path to save filtered synthetic dataset.</p> <p>Returns: - pd.DataFrame: Filtered synthetic dataset based on classifier agreement.</p> Source code in <code>src/educhateval/core.py</code> <pre><code>def filter_with_classifier(\n    self,\n    train_data: Union[str, pd.DataFrame],\n    synth_data: Union[str, pd.DataFrame],\n    text_column: str = \"text\",\n    label_column: str = \"category\",\n    split_ratio: float = 0.2,\n    training_params: list = [0.01, \"cross_entropy\", 5e-5, 8, 8, 4, 0.01],\n    tuning: bool = False,\n    tuning_params: dict = None,\n    model_save_path: str = None,\n    classifier_model_name: str = \"distilbert-base-uncased\",\n    filtered_save_path: str = None,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Train a small classifier on real labeled data and use it to filter the synthetic dataset by agreement.\n\n    Parameters:\n    - train_data (str or pd.DataFrame): Path or DataFrame of small labeled training set.\n    - synth_data (str or pd.DataFrame): Path or DataFrame of generated synthetic dataset.\n    - text_column (str): Name of the text column.\n    - label_column (str): Name of the label column.\n    - split_ratio (float): Ratio for train/test split.\n    - training_params (list): Training hyperparameters.\n    - tuning (bool): Whether to perform hyperparameter tuning using Optuna.\n    - tuning_params (dict): Optional tuning grid.\n    - model_save_path (str): Optional path to save the classifier model.\n    - classifier_model_name (str): HF model ID for the classifier.\n    - filtered_save_path (str): Optional path to save filtered synthetic dataset.\n\n    Returns:\n    - pd.DataFrame: Filtered synthetic dataset based on classifier agreement.\n    \"\"\"\n    if isinstance(train_data, pd.DataFrame) and train_data.empty:\n        raise ValueError(\"Provided training DataFrame is empty.\")\n    if isinstance(synth_data, pd.DataFrame) and synth_data.empty:\n        raise ValueError(\"Provided synthetic DataFrame is empty.\")\n\n    tokenizer = load_tokenizer(classifier_model_name)\n\n    dataset_dict, label2id = load_and_prepare_dataset(\n        train_data, text_column, label_column, split_ratio\n    )\n\n    tokenized = tokenize_dataset(dataset_dict, tokenizer)\n\n    model, trainer = train_model(\n        tokenized,\n        classifier_model_name,\n        len(label2id),\n        training_params,\n        tuning,\n        tuning_params,\n    )\n\n    trainer.evaluate()\n\n    if model_save_path:\n        save_model_and_tokenizer(model, tokenizer, model_save_path)\n\n    df_filtered = filter_synthesized_data(\n        synth_input=synth_data,\n        model=model,\n        tokenizer=tokenizer,\n        label_column=label_column,\n        save_path=filtered_save_path,\n    )\n\n    return df_filtered\n</code></pre>"},{"location":"api/api_frame_gen/#educhateval.core.FrameworkGenerator.generate_framework","title":"<code>generate_framework(prompt_path=None, prompt_dict_input=None, num_samples=500, json_out=None, csv_out=None, seed=42, temperature=0.85, top_p=0.9)</code>","text":"<p>Generate a synthetic labeled dataset from prompts using a language model.</p> <p>Either <code>prompt_path</code> (path to .py file with <code>prompt_dict</code>) or <code>prompt_dict_input</code> must be provided.</p> <p>Parameters: - prompt_path (str): Path to a Python file containing a prompt dictionary. - prompt_dict_input (dict): Prompt dictionary directly provided. - num_samples (int): Number of samples to generate per category. - json_out (str): Optional path to save JSON output. - csv_out (str): Optional path to save CSV output. - seed (int): Random seed for reproducibility.</p> <p>Returns: - pd.DataFrame: Cleaned, labeled synthetic dataset.</p> Source code in <code>src/educhateval/core.py</code> <pre><code>def generate_framework(\n    self,\n    prompt_path: str = None,\n    prompt_dict_input: dict = None,\n    num_samples: int = 500,\n    json_out: str = None,\n    csv_out: str = None,\n    seed: int = 42,\n    temperature: float = 0.85,\n    top_p: float = 0.90,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Generate a synthetic labeled dataset from prompts using a language model.\n\n    Either `prompt_path` (path to .py file with `prompt_dict`) or `prompt_dict_input` must be provided.\n\n    Parameters:\n    - prompt_path (str): Path to a Python file containing a prompt dictionary.\n    - prompt_dict_input (dict): Prompt dictionary directly provided.\n    - num_samples (int): Number of samples to generate per category.\n    - json_out (str): Optional path to save JSON output.\n    - csv_out (str): Optional path to save CSV output.\n    - seed (int): Random seed for reproducibility.\n\n    Returns:\n    - pd.DataFrame: Cleaned, labeled synthetic dataset.\n    \"\"\"\n    if not prompt_path and not prompt_dict_input:\n        raise ValueError(\n            \"You must provide either a prompt_path or prompt_dict_input.\"\n        )\n\n    set_seed(seed)\n\n    df = synthesize_dataset(\n        prompt_dict=prompt_dict_input,\n        prompt_path=prompt_path,\n        model_name=self.model_name,\n        num_samples=num_samples,\n        api_url=self.api_url,\n        json_out=json_out,\n        csv_out=csv_out,\n        temperature=temperature,\n        top_p=top_p,\n    )\n\n    return df\n</code></pre>"}]}